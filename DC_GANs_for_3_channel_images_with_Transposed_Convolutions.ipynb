{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DC_GANs_for_3_channel_images_with_Transposed_Convolutions.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdvJWJ-BCSA3",
        "colab_type": "text"
      },
      "source": [
        "# Downloading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGZCk6LtCTwc",
        "colab_type": "code",
        "outputId": "91cec01d-0e8e-48ac-d3e4-4820dd1644d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3\" --header=\"Accept-Language: en-US,en;q=0.9\" --header=\"Referer: https://www.kaggle.com/\" --header=\"Cookie: cuntwars_user_id=vFHd55haQ6; _ga=GA1.3.1069537971.1546461315; __utma=68291539.1069537971.1546461315.1547285225.1547285225.1; __utmz=68291539.1547285225.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none)\" --header=\"Connection: keep-alive\" \"https://storage.googleapis.com/kaggle-datasets/122892/296485/celebrities-100k.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1560182347&Signature=bsOOojV8YEBXx8yeRQIH2%2BFXuS1Wn%2BMttvFgeUVD17YqUuakDVfeZCDCITaqOBYQ2mpVOh1ypeRb7MS325pGnxRIEXbf9QkMKbGyufKeyU91jilDh80xmjc%2F6Rte%2B4RA3j1sOER7iahP68LQzhNFph8uNLPA%2FolyORKeiteOYZ7IMRanLbnAcgzDePfbnQGFlnz7ybQMySXILkeKBSV%2F6nwjuHosPuUZKqKIELZ1dsGTau0b7Clo3iqLm7wPS2zYldxk9%2FttLvPtbZG56xX7zFNqwWTmW4GsJ2mLzj6CdeisQTo4gh4mhhenlQ2dnxA0YWdeK3vOzEyP%2Fpo0cfP8UQ%3D%3D\" -O \"celebrities-100k.zip\" -c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-07 15:59:21--  https://storage.googleapis.com/kaggle-datasets/122892/296485/celebrities-100k.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1560182347&Signature=bsOOojV8YEBXx8yeRQIH2%2BFXuS1Wn%2BMttvFgeUVD17YqUuakDVfeZCDCITaqOBYQ2mpVOh1ypeRb7MS325pGnxRIEXbf9QkMKbGyufKeyU91jilDh80xmjc%2F6Rte%2B4RA3j1sOER7iahP68LQzhNFph8uNLPA%2FolyORKeiteOYZ7IMRanLbnAcgzDePfbnQGFlnz7ybQMySXILkeKBSV%2F6nwjuHosPuUZKqKIELZ1dsGTau0b7Clo3iqLm7wPS2zYldxk9%2FttLvPtbZG56xX7zFNqwWTmW4GsJ2mLzj6CdeisQTo4gh4mhhenlQ2dnxA0YWdeK3vOzEyP%2Fpo0cfP8UQ%3D%3D\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.142.128, 2607:f8b0:400e:c08::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.142.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 860082689 (820M) [application/zip]\n",
            "Saving to: ‘celebrities-100k.zip’\n",
            "\n",
            "celebrities-100k.zi 100%[===================>] 820.24M  77.1MB/s    in 11s     \n",
            "\n",
            "2019-06-07 15:59:32 (75.6 MB/s) - ‘celebrities-100k.zip’ saved [860082689/860082689]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uN-YT6zlCVON",
        "colab_type": "code",
        "outputId": "7b91973b-9bc4-41c2-d861-e3ff9e196f6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!unzip 'celebrities-100k.zip'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  celebrities-100k.zip\n",
            "  inflating: 100k.zip                \n",
            "  inflating: 100k.txt                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vnB-Y0NCWUc",
        "colab_type": "code",
        "outputId": "b58684cc-993f-40c8-a6ed-3fb8355b225f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!unzip '100k.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pkyw9xhChbN",
        "colab_type": "text"
      },
      "source": [
        "# Building Model and Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyossioma0ux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir 'Model Architecture transposed_convolutions'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLsOYNpxCiLt",
        "colab_type": "code",
        "outputId": "0e8dc1ad-0842-45d0-d2da-015b96b8e254",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "from keras.layers import Dense, Conv2DTranspose, Dropout, Flatten, Reshape, Input, BatchNormalization, Activation, ZeroPadding2D, MaxPooling2D, AveragePooling2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Model, Sequential\n",
        "from keras.optimizers import Adam\n",
        "from PIL import Image\n",
        "from keras.utils import plot_model\n",
        "from numpy import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sys\n",
        "import os as os\n",
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lil1919QCwtQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "noise_shape = 100\n",
        "nh = 128\n",
        "nw = 128\n",
        "nc = 3\n",
        "img_shape = (nh, nw, nc)\n",
        "\n",
        "# Carefully chosen parameters \n",
        "d_opt = Adam(lr = 0.00004, beta_1 = 0.5)\n",
        "g_opt = Adam(lr = 0.0002, beta_1 = 0.5)\n",
        "\n",
        "WEIGHT_INIT_STDDEV = 0.02\n",
        "EPSILON = 0.00005"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQl_VF-NDJX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_generator():\n",
        "  \n",
        "  # 8x8x1024\n",
        "  model = Sequential()\n",
        "  model.add(Dense(1024 * 8 * 8, input_dim = noise_shape, name = 'Dense_Layer'))\n",
        "  model.add(Reshape((8, 8, 1024) , name = \"Reshape_Layer\"))\n",
        "  model.add(LeakyReLU(name = 'Leaky_ReLU'))\n",
        "\n",
        "  # 8x8x1024 -> 16x16x512\n",
        "  model.add(Conv2DTranspose(512, kernel_size = 5, strides = 2, padding =\"same\", kernel_initializer = tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV), name = \"Transposed_Convolution\"))  \n",
        "  model.add(BatchNormalization(epsilon = EPSILON, name = \"Batch_Norm\"))\n",
        "  model.add(LeakyReLU(name = 'Leaky_ReLU_2'))\n",
        "\n",
        "  # 16x16x512 -> 32x32x256\n",
        "  model.add(Conv2DTranspose(256, kernel_size = 5, strides = 2, padding =\"same\", kernel_initializer = tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV), name = \"Transposed_Convolution_2\"))  \n",
        "  model.add(BatchNormalization(epsilon = EPSILON, name = \"Batch_Norm_2\"))\n",
        "  model.add(LeakyReLU(name = 'Leaky_ReLU_3'))\n",
        "  \n",
        "  # 32x32x256 -> 64x64x128\n",
        "  model.add(Conv2DTranspose(128, kernel_size = 5, strides = 2, padding =\"same\", kernel_initializer = tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV), name = \"Transposed_Convolution_3\"))  \n",
        "  model.add(BatchNormalization(epsilon = EPSILON, name = \"Batch_Norm_3\"))\n",
        "  model.add(LeakyReLU(name = 'Leaky_ReLU_4'))\n",
        "  \n",
        "  # 64x64x128 -> 128x128x64\n",
        "  model.add(Conv2DTranspose(64, kernel_size = 5, strides = 2, padding =\"same\", kernel_initializer = tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV), name = \"Transposed_Convolution_4\"))  \n",
        "  model.add(BatchNormalization(epsilon = EPSILON, name = \"Batch_Norm_4\"))\n",
        "  model.add(LeakyReLU(name = 'Leaky_ReLU_5'))\n",
        "  \n",
        "  # 128x128x64 -> 128x128x3\n",
        "  model.add(Conv2DTranspose(3, kernel_size = 5, strides = 1, padding =\"same\", kernel_initializer = tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV), name = \"Transposed_Convolution_5\"))  \n",
        "  \n",
        "  #TANH ACTIVATION\n",
        "  model.add(Activation(\"tanh\"))\n",
        "    \n",
        "  print(model.summary())\n",
        "  \n",
        "  #Picturizing model\n",
        "  plot_model(model, to_file='Model Architecture transposed_convolutions/generator.png')\n",
        "  \n",
        "  noise = Input(shape = (noise_shape,))\n",
        "  img = model(noise)\n",
        "  \n",
        "  return Model(inputs = noise, outputs = img, name = \"Generator\")\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCSleIL4FmvR",
        "colab_type": "code",
        "outputId": "8cd7edad-04a2-4ccc-a968-06b1d8130776",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1146
        }
      },
      "source": [
        "generator = build_generator()\n",
        "generator.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0614 14:38:19.051524 140680200431488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0614 14:38:19.054923 140680200431488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0614 14:38:19.059042 140680200431488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0614 14:38:19.127402 140680200431488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0614 14:38:19.128325 140680200431488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0614 14:38:21.987360 140680200431488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Dense_Layer (Dense)          (None, 65536)             6619136   \n",
            "_________________________________________________________________\n",
            "Reshape_Layer (Reshape)      (None, 8, 8, 1024)        0         \n",
            "_________________________________________________________________\n",
            "Leaky_ReLU (LeakyReLU)       (None, 8, 8, 1024)        0         \n",
            "_________________________________________________________________\n",
            "Transposed_Convolution (Conv (None, 16, 16, 512)       13107712  \n",
            "_________________________________________________________________\n",
            "Batch_Norm (BatchNormalizati (None, 16, 16, 512)       2048      \n",
            "_________________________________________________________________\n",
            "Leaky_ReLU_2 (LeakyReLU)     (None, 16, 16, 512)       0         \n",
            "_________________________________________________________________\n",
            "Transposed_Convolution_2 (Co (None, 32, 32, 256)       3277056   \n",
            "_________________________________________________________________\n",
            "Batch_Norm_2 (BatchNormaliza (None, 32, 32, 256)       1024      \n",
            "_________________________________________________________________\n",
            "Leaky_ReLU_3 (LeakyReLU)     (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "Transposed_Convolution_3 (Co (None, 64, 64, 128)       819328    \n",
            "_________________________________________________________________\n",
            "Batch_Norm_3 (BatchNormaliza (None, 64, 64, 128)       512       \n",
            "_________________________________________________________________\n",
            "Leaky_ReLU_4 (LeakyReLU)     (None, 64, 64, 128)       0         \n",
            "_________________________________________________________________\n",
            "Transposed_Convolution_4 (Co (None, 128, 128, 64)      204864    \n",
            "_________________________________________________________________\n",
            "Batch_Norm_4 (BatchNormaliza (None, 128, 128, 64)      256       \n",
            "_________________________________________________________________\n",
            "Leaky_ReLU_5 (LeakyReLU)     (None, 128, 128, 64)      0         \n",
            "_________________________________________________________________\n",
            "Transposed_Convolution_5 (Co (None, 128, 128, 3)       4803      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 128, 128, 3)       0         \n",
            "=================================================================\n",
            "Total params: 24,036,739\n",
            "Trainable params: 24,034,819\n",
            "Non-trainable params: 1,920\n",
            "_________________________________________________________________\n",
            "None\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "sequential_1 (Sequential)    (None, 128, 128, 3)       24036739  \n",
            "=================================================================\n",
            "Total params: 24,036,739\n",
            "Trainable params: 24,034,819\n",
            "Non-trainable params: 1,920\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF4BLKQIFoft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_discriminator():\n",
        "  model = Sequential()\n",
        "  \n",
        "  # 128*128*3 -> 64x64x64\n",
        "  model.add(Conv2D(64, kernel_size=5, strides = 2, input_shape = img_shape, padding=\"same\", kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV), name = 'Conv_Layer'))\n",
        "  model.add(BatchNormalization(epsilon = EPSILON, name = \"Batch_Norm\"))\n",
        "  model.add(LeakyReLU(name = 'Leaky_ReLU'))\n",
        "  \n",
        "  # 64x64x64-> 32x32x128\n",
        "  model.add(Conv2D(128, kernel_size=5, strides = 2, padding=\"same\", kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV), name = 'Conv_Layer_2'))\n",
        "  model.add(BatchNormalization(epsilon = EPSILON, name = \"Batch_Norm_2\"))\n",
        "  model.add(LeakyReLU(name = 'Leaky_ReLU_2'))\n",
        "  \n",
        "  # 32x32x128 -> 16x16x256\n",
        "  model.add(Conv2D(256, kernel_size=5, strides = 2, padding=\"same\", kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV), name = 'Conv_Layer_3'))\n",
        "  model.add(BatchNormalization(epsilon = EPSILON, name = \"Batch_Norm_3\"))\n",
        "  model.add(LeakyReLU(name = 'Leaky_ReLU_3'))\n",
        "  \n",
        "  # 16x16x256 -> 16x16x512\n",
        "  model.add(Conv2D(512, kernel_size=5, strides = 1, padding=\"same\", kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV), name = 'Conv_Layer_4'))\n",
        "  model.add(BatchNormalization(epsilon = EPSILON, name = \"Batch_Norm_4\"))\n",
        "  model.add(LeakyReLU(name = 'Leaky_ReLU_4'))\n",
        "  \n",
        "  # 16x16x512 -> 8x8x1024\n",
        "  model.add(Conv2D(1024, kernel_size=5, strides = 2, padding=\"same\", kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV), name = 'Conv_Layer_5'))\n",
        "  model.add(BatchNormalization(epsilon = EPSILON, name = \"Batch_Norm_5\"))\n",
        "  model.add(LeakyReLU(name = 'Leaky_ReLU_5'))\n",
        "  \n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(1, activation = None))\n",
        "  model.add(Activation(\"sigmoid\"))\n",
        "  \n",
        "  print(model.summary())\n",
        "  \n",
        "  #Picturizing model\n",
        "  plot_model(model, to_file='Model Architecture transposed_convolutions/discriminator.png')\n",
        "  \n",
        "  img = Input(shape = img_shape)\n",
        "  validity = model(img)\n",
        "  \n",
        "  return Model(inputs = img, outputs = validity, name = 'Discriminator')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4hlF9x9GvGx",
        "colab_type": "code",
        "outputId": "56842a25-5239-42ad-b8f4-9d06937f1647",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1045
        }
      },
      "source": [
        "discriminator = build_discriminator()\n",
        "discriminator.summary()\n",
        "\n",
        "discriminator.compile(loss = 'binary_crossentropy', optimizer = d_opt, metrics = ['accuracy'])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Conv_Layer (Conv2D)          (None, 64, 64, 64)        4864      \n",
            "_________________________________________________________________\n",
            "Batch_Norm (BatchNormalizati (None, 64, 64, 64)        256       \n",
            "_________________________________________________________________\n",
            "Leaky_ReLU (LeakyReLU)       (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "Conv_Layer_2 (Conv2D)        (None, 32, 32, 128)       204928    \n",
            "_________________________________________________________________\n",
            "Batch_Norm_2 (BatchNormaliza (None, 32, 32, 128)       512       \n",
            "_________________________________________________________________\n",
            "Leaky_ReLU_2 (LeakyReLU)     (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "Conv_Layer_3 (Conv2D)        (None, 16, 16, 256)       819456    \n",
            "_________________________________________________________________\n",
            "Batch_Norm_3 (BatchNormaliza (None, 16, 16, 256)       1024      \n",
            "_________________________________________________________________\n",
            "Leaky_ReLU_3 (LeakyReLU)     (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "Conv_Layer_4 (Conv2D)        (None, 16, 16, 512)       3277312   \n",
            "_________________________________________________________________\n",
            "Batch_Norm_4 (BatchNormaliza (None, 16, 16, 512)       2048      \n",
            "_________________________________________________________________\n",
            "Leaky_ReLU_4 (LeakyReLU)     (None, 16, 16, 512)       0         \n",
            "_________________________________________________________________\n",
            "Conv_Layer_5 (Conv2D)        (None, 8, 8, 1024)        13108224  \n",
            "_________________________________________________________________\n",
            "Batch_Norm_5 (BatchNormaliza (None, 8, 8, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "Leaky_ReLU_5 (LeakyReLU)     (None, 8, 8, 1024)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 65536)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 65537     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 17,488,257\n",
            "Trainable params: 17,484,289\n",
            "Non-trainable params: 3,968\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0614 14:38:24.124400 140680200431488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0614 14:38:24.135329 140680200431488 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 128, 128, 3)       0         \n",
            "_________________________________________________________________\n",
            "sequential_2 (Sequential)    (None, 1)                 17488257  \n",
            "=================================================================\n",
            "Total params: 17,488,257\n",
            "Trainable params: 17,484,289\n",
            "Non-trainable params: 3,968\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUsKNEB5GxMN",
        "colab_type": "code",
        "outputId": "32b1f177-4ee7-4b55-cb7e-56c859fa72a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "z = Input(shape = (noise_shape,), name = \"Input_Noise\")\n",
        "img = generator(z)\n",
        "\n",
        "discriminator.trainable = False\n",
        "valid = discriminator(img)\n",
        "\n",
        "combined = Model(z, valid)\n",
        "combined.compile(loss = 'binary_crossentropy', optimizer = g_opt)\n",
        "combined.summary()\n",
        "\n",
        "#Picturizing model\n",
        "plot_model(combined, to_file='Model Architecture transposed_convolutions/combined.png')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Input_Noise (InputLayer)     (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "Generator (Model)            (None, 128, 128, 3)       24036739  \n",
            "_________________________________________________________________\n",
            "Discriminator (Model)        (None, 1)                 17488257  \n",
            "=================================================================\n",
            "Total params: 41,524,996\n",
            "Trainable params: 24,034,819\n",
            "Non-trainable params: 17,490,177\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pmftjb4AJOfT",
        "colab_type": "code",
        "outputId": "bed42977-2a8b-4506-d9b4-df6ca2c6a4ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        }
      },
      "source": [
        "print(generator.summary())\n",
        "print(discriminator.summary())\n",
        "print(combined.summary())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "sequential_1 (Sequential)    (None, 128, 128, 3)       24036739  \n",
            "=================================================================\n",
            "Total params: 24,036,739\n",
            "Trainable params: 24,034,819\n",
            "Non-trainable params: 1,920\n",
            "_________________________________________________________________\n",
            "None\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 128, 128, 3)       0         \n",
            "_________________________________________________________________\n",
            "sequential_2 (Sequential)    (None, 1)                 17488257  \n",
            "=================================================================\n",
            "Total params: 34,972,546\n",
            "Trainable params: 17,484,289\n",
            "Non-trainable params: 17,488,257\n",
            "_________________________________________________________________\n",
            "None\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Input_Noise (InputLayer)     (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "Generator (Model)            (None, 128, 128, 3)       24036739  \n",
            "_________________________________________________________________\n",
            "Discriminator (Model)        (None, 1)                 17488257  \n",
            "=================================================================\n",
            "Total params: 41,524,996\n",
            "Trainable params: 24,034,819\n",
            "Non-trainable params: 17,490,177\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYgfpyLQJQWU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batch(width, height):\n",
        "  image_files = os.listdir('/content/100k/')[:40000]\n",
        "  data_batch = []\n",
        "  \n",
        "  for file in image_files:\n",
        "    img = Image.open('/content/100k/'+file).resize([width, height])\n",
        "    data_batch.append(np.array(img.convert('RGB')))\n",
        "    \n",
        "  data_batch = np.array(data_batch)\n",
        "  print(data_batch.shape)\n",
        "  return data_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgdvtJifJSPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(batch_size = 128, epochs = 15830):\n",
        "  \n",
        "  \n",
        "  valid = np.ones((batch_size, 1))\n",
        "  fakes = np.zeros((batch_size, 1))\n",
        "  \n",
        "  X_train = get_batch(nw, nh)\n",
        "  X_train = (X_train.astype(np.float32) / 127.5) - 1.0\n",
        "  \n",
        "  #adding noise to input images\n",
        "  for i in range(len(X_train)):\n",
        "    X_train[i] = X_train[i] + np.random.normal(size = X_train[i].shape, loc = 0.0, scale = random.uniform(0.0, 0.1)).astype(np.float32)\n",
        "  \n",
        "  for epoch in range(70000, epochs):\n",
        "    \n",
        "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "    imgs = X_train[idx]\n",
        "    \n",
        "    noise = np.random.normal(0, 1, (batch_size, noise_shape))\n",
        "    \n",
        "    gen_imgs = generator.predict(noise)\n",
        "\n",
        "    #Training Discriminator\n",
        "    d_loss_real = discriminator.train_on_batch(imgs, valid*random.uniform(0.9, 1.0))\n",
        "    d_loss_fake = discriminator.train_on_batch(gen_imgs, fakes)\n",
        "    d_loss = 0.5*np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "    #Training Generator\n",
        "    g_loss = combined.train_on_batch(noise, valid)\n",
        "\n",
        "    #Progress\n",
        "    if (epoch+1)%50 == 0:\n",
        "      print(\"epoch: \" + str(epoch+1) + \" \" + \"D_Loss = \" + str(d_loss[0]) + \" \" + \"acc: \" + str(d_loss[1]*100) + \" \" +  \"G_Loss = \" + str(g_loss))\n",
        "    \n",
        "    if (epoch+1)%10000 == 0 or epoch+1 == 1000:\n",
        "      save_imgs(epoch+1)\n",
        "      generator.save_weights('generator.h5')\n",
        "      discriminator.save_weights('discriminator.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRWtENiJKOHR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir 'images_transposed_convolutions'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ix5Gr85jKP8s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_imgs(epoch):\n",
        "  r, c = 3, 3\n",
        "  noise = np.random.normal(0, 1, (r * c, noise_shape))\n",
        "  gen_imgs = generator.predict(noise)\n",
        "\n",
        "  # Rescale images 0 - 1\n",
        "  gen_imgs = (1/2.5) * gen_imgs + 0.5\n",
        "  \n",
        "  fig, axs = plt.subplots(r, c)\n",
        "  cnt = 0\n",
        "  for i in range(r):\n",
        "      for j in range(c):\n",
        "          axs[i,j].imshow(gen_imgs[cnt, :,:,:])\n",
        "          axs[i,j].axis('off')\n",
        "          cnt += 1\n",
        "  fig.savefig(\"images_transposed_convolutions/%d.png\" % epoch)\n",
        "  plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0_VyXpMKRbU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#generator.load_weights('generator.h5')\n",
        "#discriminator.load_weights('discriminator.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHO9ZhemKTR8",
        "colab_type": "code",
        "outputId": "92ebf850-b4e1-4e13-9b3b-f7553646ec1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8962
        }
      },
      "source": [
        "train(64, 100000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(40000, 128, 128, 3)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 70050 D_Loss = 0.10552743 acc: 50.0 G_Loss = 1.0561694\n",
            "epoch: 70100 D_Loss = 0.04420022 acc: 50.0 G_Loss = 0.20355316\n",
            "epoch: 70150 D_Loss = 0.06738627 acc: 50.0 G_Loss = 0.09743933\n",
            "epoch: 70200 D_Loss = 0.044008028 acc: 50.0 G_Loss = 0.17431289\n",
            "epoch: 70250 D_Loss = 0.14734924 acc: 50.0 G_Loss = 0.2036498\n",
            "epoch: 70300 D_Loss = 0.21408778 acc: 50.0 G_Loss = 1.7051105\n",
            "epoch: 70350 D_Loss = 0.07258231 acc: 50.0 G_Loss = 0.02706325\n",
            "epoch: 70400 D_Loss = 0.111648515 acc: 50.0 G_Loss = 0.024665382\n",
            "epoch: 70450 D_Loss = 0.052209817 acc: 50.0 G_Loss = 0.036941327\n",
            "epoch: 70500 D_Loss = 0.08552354 acc: 50.0 G_Loss = 0.027282452\n",
            "epoch: 70550 D_Loss = 0.11972844 acc: 50.0 G_Loss = 0.28563154\n",
            "epoch: 70600 D_Loss = 0.15278652 acc: 50.0 G_Loss = 0.20852631\n",
            "epoch: 70650 D_Loss = 0.072271794 acc: 50.0 G_Loss = 0.21463333\n",
            "epoch: 70700 D_Loss = 0.17997192 acc: 50.0 G_Loss = 0.9117613\n",
            "epoch: 70750 D_Loss = 0.11500873 acc: 50.0 G_Loss = 0.33619756\n",
            "epoch: 70800 D_Loss = 0.1691483 acc: 50.0 G_Loss = 0.09041442\n",
            "epoch: 70850 D_Loss = 0.039105374 acc: 50.0 G_Loss = 0.1056879\n",
            "epoch: 70900 D_Loss = 0.11930175 acc: 50.0 G_Loss = 0.05965762\n",
            "epoch: 70950 D_Loss = 0.04798291 acc: 50.0 G_Loss = 0.060406163\n",
            "epoch: 71000 D_Loss = 0.13113275 acc: 50.0 G_Loss = 0.54732454\n",
            "epoch: 71050 D_Loss = 0.13027145 acc: 50.0 G_Loss = 0.09117185\n",
            "epoch: 71100 D_Loss = 0.10251447 acc: 50.0 G_Loss = 0.097271815\n",
            "epoch: 71150 D_Loss = 0.098009124 acc: 50.0 G_Loss = 0.80707026\n",
            "epoch: 71200 D_Loss = 0.077734284 acc: 50.0 G_Loss = 0.052307896\n",
            "epoch: 71250 D_Loss = 0.073186934 acc: 50.0 G_Loss = 0.027378948\n",
            "epoch: 71300 D_Loss = 0.088013746 acc: 50.0 G_Loss = 0.012937184\n",
            "epoch: 71350 D_Loss = 0.12356136 acc: 50.0 G_Loss = 0.41358203\n",
            "epoch: 71400 D_Loss = 0.09639254 acc: 50.0 G_Loss = 0.10056698\n",
            "epoch: 71450 D_Loss = 0.19890136 acc: 50.0 G_Loss = 0.1513675\n",
            "epoch: 71500 D_Loss = 0.122404516 acc: 50.0 G_Loss = 0.104326084\n",
            "epoch: 71550 D_Loss = 0.06589309 acc: 50.0 G_Loss = 0.03907313\n",
            "epoch: 71600 D_Loss = 0.20917213 acc: 50.0 G_Loss = 0.21698867\n",
            "epoch: 71650 D_Loss = 0.12354443 acc: 50.0 G_Loss = 0.15583815\n",
            "epoch: 71700 D_Loss = 0.13440344 acc: 50.0 G_Loss = 0.1285342\n",
            "epoch: 71750 D_Loss = 0.034250077 acc: 50.0 G_Loss = 0.14081278\n",
            "epoch: 71800 D_Loss = 0.11766895 acc: 50.0 G_Loss = 0.13891084\n",
            "epoch: 71850 D_Loss = 0.07279098 acc: 50.0 G_Loss = 0.15810886\n",
            "epoch: 71900 D_Loss = 0.1417285 acc: 50.0 G_Loss = 0.08955116\n",
            "epoch: 71950 D_Loss = 0.077932656 acc: 50.0 G_Loss = 0.19431993\n",
            "epoch: 72000 D_Loss = 0.1296136 acc: 50.0 G_Loss = 0.12621832\n",
            "epoch: 72050 D_Loss = 0.15744719 acc: 50.0 G_Loss = 0.28457257\n",
            "epoch: 72100 D_Loss = 0.13209412 acc: 50.0 G_Loss = 0.18321359\n",
            "epoch: 72150 D_Loss = 0.12876247 acc: 50.0 G_Loss = 0.10840548\n",
            "epoch: 72200 D_Loss = 0.21465842 acc: 50.0 G_Loss = 0.05562351\n",
            "epoch: 72250 D_Loss = 0.14159133 acc: 50.0 G_Loss = 0.096518345\n",
            "epoch: 72300 D_Loss = 0.11623202 acc: 50.0 G_Loss = 0.074837774\n",
            "epoch: 72350 D_Loss = 0.099080496 acc: 50.0 G_Loss = 0.17796336\n",
            "epoch: 72400 D_Loss = 0.09801088 acc: 50.0 G_Loss = 0.07091856\n",
            "epoch: 72450 D_Loss = 0.1444486 acc: 50.0 G_Loss = 0.0982915\n",
            "epoch: 72500 D_Loss = 0.13593449 acc: 50.0 G_Loss = 3.039823\n",
            "epoch: 72550 D_Loss = 0.18634692 acc: 50.0 G_Loss = 0.29829043\n",
            "epoch: 72600 D_Loss = 0.20198864 acc: 50.0 G_Loss = 0.13887177\n",
            "epoch: 72650 D_Loss = 0.078988545 acc: 50.0 G_Loss = 0.012525633\n",
            "epoch: 72700 D_Loss = 0.05714545 acc: 50.0 G_Loss = 0.023414647\n",
            "epoch: 72750 D_Loss = 0.042659983 acc: 50.0 G_Loss = 0.034230486\n",
            "epoch: 72800 D_Loss = 0.179689 acc: 50.0 G_Loss = 0.5639987\n",
            "epoch: 72850 D_Loss = 0.11733075 acc: 50.0 G_Loss = 0.06448048\n",
            "epoch: 72900 D_Loss = 0.062346354 acc: 50.0 G_Loss = 0.05299632\n",
            "epoch: 72950 D_Loss = 0.12732866 acc: 50.0 G_Loss = 2.1952515\n",
            "epoch: 73000 D_Loss = 0.03402707 acc: 50.0 G_Loss = 0.061804816\n",
            "epoch: 73050 D_Loss = 0.07840718 acc: 50.0 G_Loss = 0.18484777\n",
            "epoch: 73100 D_Loss = 0.12561335 acc: 50.0 G_Loss = 0.02782916\n",
            "epoch: 73150 D_Loss = 0.045808405 acc: 50.0 G_Loss = 0.03154448\n",
            "epoch: 73200 D_Loss = 0.27444032 acc: 49.21875 G_Loss = 7.815439\n",
            "epoch: 73250 D_Loss = 0.16876207 acc: 50.0 G_Loss = 0.24803784\n",
            "epoch: 73300 D_Loss = 0.073433794 acc: 50.0 G_Loss = 0.131951\n",
            "epoch: 73350 D_Loss = 0.098574564 acc: 50.0 G_Loss = 0.06640573\n",
            "epoch: 73400 D_Loss = 0.062116094 acc: 50.0 G_Loss = 0.020213\n",
            "epoch: 73450 D_Loss = 0.1291325 acc: 50.0 G_Loss = 0.15376872\n",
            "epoch: 73500 D_Loss = 0.13277012 acc: 50.0 G_Loss = 0.39911824\n",
            "epoch: 73550 D_Loss = 0.034916587 acc: 50.0 G_Loss = 0.111095786\n",
            "epoch: 73600 D_Loss = 0.05986499 acc: 50.0 G_Loss = 0.08786623\n",
            "epoch: 73650 D_Loss = 0.11232897 acc: 50.0 G_Loss = 0.1955849\n",
            "epoch: 73700 D_Loss = 0.10485229 acc: 50.0 G_Loss = 2.9329038\n",
            "epoch: 73750 D_Loss = 0.049890622 acc: 50.0 G_Loss = 0.10742742\n",
            "epoch: 73800 D_Loss = 0.102648646 acc: 50.0 G_Loss = 0.047547087\n",
            "epoch: 73850 D_Loss = 0.09888832 acc: 50.0 G_Loss = 0.48747572\n",
            "epoch: 73900 D_Loss = 0.07500046 acc: 50.0 G_Loss = 0.0411518\n",
            "epoch: 73950 D_Loss = 0.149776 acc: 50.0 G_Loss = 0.016982064\n",
            "epoch: 74000 D_Loss = 0.18442307 acc: 50.0 G_Loss = 0.068613134\n",
            "epoch: 74050 D_Loss = 0.06878482 acc: 50.0 G_Loss = 0.08273801\n",
            "epoch: 74100 D_Loss = 0.06537794 acc: 50.0 G_Loss = 0.015203888\n",
            "epoch: 74150 D_Loss = 0.13648564 acc: 50.0 G_Loss = 0.01723323\n",
            "epoch: 74200 D_Loss = 0.17735791 acc: 50.0 G_Loss = 0.9927282\n",
            "epoch: 74250 D_Loss = 0.12053376 acc: 50.0 G_Loss = 0.3594003\n",
            "epoch: 74300 D_Loss = 0.12938525 acc: 50.0 G_Loss = 0.054296646\n",
            "epoch: 74350 D_Loss = 0.039456107 acc: 50.0 G_Loss = 0.44650596\n",
            "epoch: 74400 D_Loss = 0.13013543 acc: 50.0 G_Loss = 0.04156328\n",
            "epoch: 74450 D_Loss = 0.14355695 acc: 50.0 G_Loss = 0.21789804\n",
            "epoch: 74500 D_Loss = 0.061654642 acc: 50.0 G_Loss = 0.11563267\n",
            "epoch: 74550 D_Loss = 0.07211935 acc: 50.0 G_Loss = 0.28713766\n",
            "epoch: 74600 D_Loss = 0.05205907 acc: 50.0 G_Loss = 0.07246901\n",
            "epoch: 74650 D_Loss = 0.1234565 acc: 50.0 G_Loss = 0.13225774\n",
            "epoch: 74700 D_Loss = 0.061716437 acc: 50.0 G_Loss = 0.7760254\n",
            "epoch: 74750 D_Loss = 0.14007238 acc: 50.0 G_Loss = 0.18499191\n",
            "epoch: 74800 D_Loss = 0.05613565 acc: 50.0 G_Loss = 0.25173137\n",
            "epoch: 74850 D_Loss = 0.10038982 acc: 50.0 G_Loss = 0.11576316\n",
            "epoch: 74900 D_Loss = 0.14725262 acc: 50.0 G_Loss = 0.11546008\n",
            "epoch: 74950 D_Loss = 0.14933942 acc: 50.0 G_Loss = 0.33292314\n",
            "epoch: 75000 D_Loss = 0.1824278 acc: 50.0 G_Loss = 1.2017605\n",
            "epoch: 75050 D_Loss = 0.16551991 acc: 50.0 G_Loss = 1.7880138\n",
            "epoch: 75100 D_Loss = 0.15106961 acc: 50.0 G_Loss = 0.17158484\n",
            "epoch: 75150 D_Loss = 0.036792383 acc: 50.0 G_Loss = 0.1813414\n",
            "epoch: 75200 D_Loss = 0.10678641 acc: 50.0 G_Loss = 0.23204115\n",
            "epoch: 75250 D_Loss = 0.093040496 acc: 50.0 G_Loss = 1.1535217\n",
            "epoch: 75300 D_Loss = 0.14134169 acc: 50.0 G_Loss = 0.10995552\n",
            "epoch: 75350 D_Loss = 0.08177144 acc: 50.0 G_Loss = 0.045953263\n",
            "epoch: 75400 D_Loss = 0.11568095 acc: 50.0 G_Loss = 0.07623118\n",
            "epoch: 75450 D_Loss = 0.16923194 acc: 50.0 G_Loss = 2.1875644\n",
            "epoch: 75500 D_Loss = 0.12286848 acc: 50.0 G_Loss = 0.12518917\n",
            "epoch: 75550 D_Loss = 0.1126275 acc: 50.0 G_Loss = 1.5591257\n",
            "epoch: 75600 D_Loss = 0.14821185 acc: 50.0 G_Loss = 0.260107\n",
            "epoch: 75650 D_Loss = 0.17647627 acc: 50.0 G_Loss = 0.60909796\n",
            "epoch: 75700 D_Loss = 0.14141169 acc: 50.0 G_Loss = 0.10039836\n",
            "epoch: 75750 D_Loss = 0.051009446 acc: 50.0 G_Loss = 6.3805213\n",
            "epoch: 75800 D_Loss = 0.08495774 acc: 50.0 G_Loss = 0.17988864\n",
            "epoch: 75850 D_Loss = 0.06545186 acc: 50.0 G_Loss = 0.039215565\n",
            "epoch: 75900 D_Loss = 0.17147644 acc: 50.0 G_Loss = 0.0661046\n",
            "epoch: 75950 D_Loss = 0.1384418 acc: 50.0 G_Loss = 0.07393351\n",
            "epoch: 76000 D_Loss = 0.12101722 acc: 50.0 G_Loss = 0.7088034\n",
            "epoch: 76050 D_Loss = 0.117768265 acc: 50.0 G_Loss = 1.137434\n",
            "epoch: 76100 D_Loss = 0.057844497 acc: 50.0 G_Loss = 0.05396182\n",
            "epoch: 76150 D_Loss = 0.15757564 acc: 50.0 G_Loss = 0.11521354\n",
            "epoch: 76200 D_Loss = 0.0708357 acc: 50.0 G_Loss = 0.5053692\n",
            "epoch: 76250 D_Loss = 0.10185773 acc: 50.0 G_Loss = 0.06340874\n",
            "epoch: 76300 D_Loss = 0.20164764 acc: 50.0 G_Loss = 1.8189256\n",
            "epoch: 76350 D_Loss = 0.07767961 acc: 50.0 G_Loss = 0.05661243\n",
            "epoch: 76400 D_Loss = 0.12659757 acc: 50.0 G_Loss = 0.1035891\n",
            "epoch: 76450 D_Loss = 0.18861426 acc: 50.0 G_Loss = 0.20807005\n",
            "epoch: 76500 D_Loss = 0.080513544 acc: 50.0 G_Loss = 1.665877\n",
            "epoch: 76550 D_Loss = 0.13097195 acc: 50.0 G_Loss = 0.15282816\n",
            "epoch: 76600 D_Loss = 0.1637303 acc: 50.0 G_Loss = 0.15152353\n",
            "epoch: 76650 D_Loss = 0.11597743 acc: 50.0 G_Loss = 0.06911163\n",
            "epoch: 76700 D_Loss = 0.18720286 acc: 50.0 G_Loss = 2.2048695\n",
            "epoch: 76750 D_Loss = 0.04962487 acc: 50.0 G_Loss = 0.08481521\n",
            "epoch: 76800 D_Loss = 0.08312431 acc: 50.0 G_Loss = 0.08187048\n",
            "epoch: 76850 D_Loss = 0.060181994 acc: 50.0 G_Loss = 0.06851412\n",
            "epoch: 76900 D_Loss = 0.16377863 acc: 50.0 G_Loss = 0.31996495\n",
            "epoch: 76950 D_Loss = 0.13708168 acc: 50.0 G_Loss = 0.2814362\n",
            "epoch: 77000 D_Loss = 0.16903076 acc: 50.0 G_Loss = 0.052615486\n",
            "epoch: 77050 D_Loss = 0.11836692 acc: 50.0 G_Loss = 0.106130905\n",
            "epoch: 77100 D_Loss = 0.07225014 acc: 50.0 G_Loss = 0.2719952\n",
            "epoch: 77150 D_Loss = 0.1272116 acc: 50.0 G_Loss = 0.050064694\n",
            "epoch: 77200 D_Loss = 0.06848464 acc: 50.0 G_Loss = 0.01736731\n",
            "epoch: 77250 D_Loss = 0.0657387 acc: 50.0 G_Loss = 0.08488354\n",
            "epoch: 77300 D_Loss = 0.21352011 acc: 50.0 G_Loss = 0.09385674\n",
            "epoch: 77350 D_Loss = 0.07836709 acc: 50.0 G_Loss = 0.27360433\n",
            "epoch: 77400 D_Loss = 0.17420568 acc: 50.0 G_Loss = 0.16961583\n",
            "epoch: 77450 D_Loss = 0.18040752 acc: 50.0 G_Loss = 1.4291604\n",
            "epoch: 77500 D_Loss = 0.1015977 acc: 50.0 G_Loss = 0.32306966\n",
            "epoch: 77550 D_Loss = 0.11364988 acc: 50.0 G_Loss = 0.05110279\n",
            "epoch: 77600 D_Loss = 0.08795453 acc: 50.0 G_Loss = 0.055585124\n",
            "epoch: 77650 D_Loss = 0.099145375 acc: 50.0 G_Loss = 0.013672609\n",
            "epoch: 77700 D_Loss = 0.15319498 acc: 50.0 G_Loss = 0.0394182\n",
            "epoch: 77750 D_Loss = 0.060556665 acc: 50.0 G_Loss = 0.028775867\n",
            "epoch: 77800 D_Loss = 0.14435834 acc: 50.0 G_Loss = 0.087687075\n",
            "epoch: 77850 D_Loss = 0.038803972 acc: 50.0 G_Loss = 2.8391335\n",
            "epoch: 77900 D_Loss = 0.035672575 acc: 50.0 G_Loss = 0.26011127\n",
            "epoch: 77950 D_Loss = 0.08640942 acc: 50.0 G_Loss = 0.09733758\n",
            "epoch: 78000 D_Loss = 0.082904205 acc: 50.0 G_Loss = 0.05681221\n",
            "epoch: 78050 D_Loss = 0.12675962 acc: 50.0 G_Loss = 0.11071751\n",
            "epoch: 78100 D_Loss = 0.39181423 acc: 50.0 G_Loss = 4.0961685\n",
            "epoch: 78150 D_Loss = 0.18713078 acc: 50.0 G_Loss = 0.22912523\n",
            "epoch: 78200 D_Loss = 0.20185585 acc: 50.0 G_Loss = 0.1983104\n",
            "epoch: 78250 D_Loss = 0.06538657 acc: 50.0 G_Loss = 0.08357963\n",
            "epoch: 78300 D_Loss = 0.17263058 acc: 50.0 G_Loss = 0.21919423\n",
            "epoch: 78350 D_Loss = 0.100362286 acc: 50.0 G_Loss = 0.02620675\n",
            "epoch: 78400 D_Loss = 0.04737008 acc: 50.0 G_Loss = 0.02161792\n",
            "epoch: 78450 D_Loss = 0.123499006 acc: 50.0 G_Loss = 2.5590262\n",
            "epoch: 78500 D_Loss = 0.089961454 acc: 50.0 G_Loss = 0.051995717\n",
            "epoch: 78550 D_Loss = 0.15906994 acc: 50.0 G_Loss = 0.11316311\n",
            "epoch: 78600 D_Loss = 0.03353075 acc: 50.0 G_Loss = 0.012544703\n",
            "epoch: 78650 D_Loss = 0.079980336 acc: 50.0 G_Loss = 0.06279409\n",
            "epoch: 78700 D_Loss = 0.05033831 acc: 50.0 G_Loss = 0.07742621\n",
            "epoch: 78750 D_Loss = 0.034790963 acc: 50.0 G_Loss = 2.3804474\n",
            "epoch: 78800 D_Loss = 0.17515808 acc: 50.0 G_Loss = 0.31943506\n",
            "epoch: 78850 D_Loss = 0.035647675 acc: 50.0 G_Loss = 0.035773735\n",
            "epoch: 78900 D_Loss = 0.119980566 acc: 50.0 G_Loss = 0.022329489\n",
            "epoch: 78950 D_Loss = 0.1456848 acc: 50.0 G_Loss = 0.08764593\n",
            "epoch: 79000 D_Loss = 0.1801723 acc: 50.0 G_Loss = 0.17481053\n",
            "epoch: 79050 D_Loss = 0.14449425 acc: 50.0 G_Loss = 0.06463506\n",
            "epoch: 79100 D_Loss = 0.21954492 acc: 50.0 G_Loss = 0.6019591\n",
            "epoch: 79150 D_Loss = 0.17583112 acc: 50.0 G_Loss = 0.62849855\n",
            "epoch: 79200 D_Loss = 0.059917897 acc: 50.0 G_Loss = 0.05796314\n",
            "epoch: 79250 D_Loss = 0.12909307 acc: 50.0 G_Loss = 0.027663248\n",
            "epoch: 79300 D_Loss = 0.2195007 acc: 50.0 G_Loss = 0.07966461\n",
            "epoch: 79350 D_Loss = 0.14161545 acc: 50.0 G_Loss = 0.052263968\n",
            "epoch: 79400 D_Loss = 0.09125026 acc: 50.0 G_Loss = 0.061945014\n",
            "epoch: 79450 D_Loss = 0.061887078 acc: 50.0 G_Loss = 0.58570063\n",
            "epoch: 79500 D_Loss = 0.018686306 acc: 50.0 G_Loss = 0.04341565\n",
            "epoch: 79550 D_Loss = 0.14671488 acc: 50.0 G_Loss = 0.03134823\n",
            "epoch: 79600 D_Loss = 0.050603315 acc: 50.0 G_Loss = 0.040113155\n",
            "epoch: 79650 D_Loss = 0.10325426 acc: 50.0 G_Loss = 0.27025285\n",
            "epoch: 79700 D_Loss = 0.08979467 acc: 50.0 G_Loss = 0.17503461\n",
            "epoch: 79750 D_Loss = 0.14972806 acc: 50.0 G_Loss = 2.4115293\n",
            "epoch: 79800 D_Loss = 0.10900675 acc: 50.0 G_Loss = 1.2112553\n",
            "epoch: 79850 D_Loss = 0.13718767 acc: 50.0 G_Loss = 1.7117201\n",
            "epoch: 79900 D_Loss = 0.082454294 acc: 50.0 G_Loss = 0.10757695\n",
            "epoch: 79950 D_Loss = 0.15126988 acc: 50.0 G_Loss = 0.11071775\n",
            "epoch: 80000 D_Loss = 0.13010964 acc: 50.0 G_Loss = 0.15795073\n",
            "epoch: 80050 D_Loss = 0.057875786 acc: 50.0 G_Loss = 0.04921284\n",
            "epoch: 80100 D_Loss = 0.101221785 acc: 50.0 G_Loss = 0.04624574\n",
            "epoch: 80150 D_Loss = 0.022650449 acc: 50.0 G_Loss = 0.043188903\n",
            "epoch: 80200 D_Loss = 0.08478906 acc: 50.0 G_Loss = 0.030912966\n",
            "epoch: 80250 D_Loss = 0.15068434 acc: 50.0 G_Loss = 2.974673\n",
            "epoch: 80300 D_Loss = 0.056077503 acc: 50.0 G_Loss = 0.0783974\n",
            "epoch: 80350 D_Loss = 0.17690626 acc: 50.0 G_Loss = 0.1566121\n",
            "epoch: 80400 D_Loss = 0.16670524 acc: 50.0 G_Loss = 0.5076133\n",
            "epoch: 80450 D_Loss = 0.12708525 acc: 50.0 G_Loss = 0.38068035\n",
            "epoch: 80500 D_Loss = 0.22280206 acc: 50.0 G_Loss = 0.13060835\n",
            "epoch: 80550 D_Loss = 0.12674026 acc: 50.0 G_Loss = 0.20913619\n",
            "epoch: 80600 D_Loss = 0.09083583 acc: 50.0 G_Loss = 0.14237328\n",
            "epoch: 80650 D_Loss = 0.07868563 acc: 50.0 G_Loss = 0.15896621\n",
            "epoch: 80700 D_Loss = 0.21243355 acc: 50.0 G_Loss = 0.059703927\n",
            "epoch: 80750 D_Loss = 0.10194624 acc: 50.0 G_Loss = 0.09423204\n",
            "epoch: 80800 D_Loss = 0.11950262 acc: 50.0 G_Loss = 0.29110903\n",
            "epoch: 80850 D_Loss = 0.18518938 acc: 50.0 G_Loss = 0.2130196\n",
            "epoch: 80900 D_Loss = 0.1293246 acc: 50.0 G_Loss = 0.39448005\n",
            "epoch: 80950 D_Loss = 0.17222153 acc: 50.0 G_Loss = 1.725915\n",
            "epoch: 81000 D_Loss = 0.18473752 acc: 50.0 G_Loss = 0.2835791\n",
            "epoch: 81050 D_Loss = 0.043109126 acc: 50.0 G_Loss = 0.01934873\n",
            "epoch: 81100 D_Loss = 0.08751428 acc: 50.0 G_Loss = 0.20958304\n",
            "epoch: 81150 D_Loss = 0.17424388 acc: 50.0 G_Loss = 0.10471099\n",
            "epoch: 81200 D_Loss = 0.15403426 acc: 50.0 G_Loss = 0.55970395\n",
            "epoch: 81250 D_Loss = 0.1270682 acc: 50.0 G_Loss = 2.4758966\n",
            "epoch: 81300 D_Loss = 0.15127078 acc: 50.0 G_Loss = 0.12911786\n",
            "epoch: 81350 D_Loss = 0.08755374 acc: 50.0 G_Loss = 0.028162172\n",
            "epoch: 81400 D_Loss = 0.2806471 acc: 50.0 G_Loss = 0.20555675\n",
            "epoch: 81450 D_Loss = 0.10026257 acc: 50.0 G_Loss = 0.06301205\n",
            "epoch: 81500 D_Loss = 0.06617079 acc: 50.0 G_Loss = 0.033749588\n",
            "epoch: 81550 D_Loss = 0.050948247 acc: 50.0 G_Loss = 0.03860019\n",
            "epoch: 81600 D_Loss = 0.06772294 acc: 50.0 G_Loss = 0.029851545\n",
            "epoch: 81650 D_Loss = 0.06431612 acc: 50.0 G_Loss = 0.2014658\n",
            "epoch: 81700 D_Loss = 0.1788516 acc: 50.0 G_Loss = 0.6183821\n",
            "epoch: 81750 D_Loss = 0.21261348 acc: 50.0 G_Loss = 0.06433031\n",
            "epoch: 81800 D_Loss = 0.19012636 acc: 50.0 G_Loss = 0.09969112\n",
            "epoch: 81850 D_Loss = 0.1831713 acc: 50.0 G_Loss = 0.19690353\n",
            "epoch: 81900 D_Loss = 0.069270894 acc: 50.0 G_Loss = 0.18995956\n",
            "epoch: 81950 D_Loss = 0.14128052 acc: 50.0 G_Loss = 0.84653\n",
            "epoch: 82000 D_Loss = 0.066420116 acc: 50.0 G_Loss = 0.05736617\n",
            "epoch: 82050 D_Loss = 0.07620061 acc: 50.0 G_Loss = 0.015742708\n",
            "epoch: 82100 D_Loss = 0.11659991 acc: 50.0 G_Loss = 0.015962936\n",
            "epoch: 82150 D_Loss = 0.112617075 acc: 50.0 G_Loss = 0.046091635\n",
            "epoch: 82200 D_Loss = 0.18136528 acc: 50.0 G_Loss = 0.42456925\n",
            "epoch: 82250 D_Loss = 0.18662694 acc: 50.0 G_Loss = 0.2135106\n",
            "epoch: 82300 D_Loss = 0.04286691 acc: 50.0 G_Loss = 0.201841\n",
            "epoch: 82350 D_Loss = 0.08486454 acc: 50.0 G_Loss = 0.11483201\n",
            "epoch: 82400 D_Loss = 0.085295476 acc: 50.0 G_Loss = 0.23594107\n",
            "epoch: 82450 D_Loss = 0.05220324 acc: 50.0 G_Loss = 1.0672529\n",
            "epoch: 82500 D_Loss = 0.1580857 acc: 50.0 G_Loss = 0.14190601\n",
            "epoch: 82550 D_Loss = 0.15805389 acc: 50.0 G_Loss = 0.3231622\n",
            "epoch: 82600 D_Loss = 0.11174515 acc: 50.0 G_Loss = 0.3059245\n",
            "epoch: 82650 D_Loss = 0.1570245 acc: 50.0 G_Loss = 0.79666775\n",
            "epoch: 82700 D_Loss = 0.18031764 acc: 50.0 G_Loss = 0.1047882\n",
            "epoch: 82750 D_Loss = 0.087058894 acc: 50.0 G_Loss = 0.05633971\n",
            "epoch: 82800 D_Loss = 0.10488977 acc: 50.0 G_Loss = 0.057769265\n",
            "epoch: 82850 D_Loss = 0.15931472 acc: 50.0 G_Loss = 0.21045823\n",
            "epoch: 82900 D_Loss = 0.18687804 acc: 50.0 G_Loss = 4.1087112\n",
            "epoch: 82950 D_Loss = 0.27153683 acc: 50.0 G_Loss = 0.5325592\n",
            "epoch: 83000 D_Loss = 0.087717205 acc: 50.0 G_Loss = 0.057839084\n",
            "epoch: 83050 D_Loss = 0.16435756 acc: 50.0 G_Loss = 0.06559472\n",
            "epoch: 83100 D_Loss = 0.07506421 acc: 50.0 G_Loss = 0.02071981\n",
            "epoch: 83150 D_Loss = 0.10044376 acc: 50.0 G_Loss = 0.044049248\n",
            "epoch: 83200 D_Loss = 0.050034706 acc: 50.0 G_Loss = 0.38385898\n",
            "epoch: 83250 D_Loss = 0.1815865 acc: 50.0 G_Loss = 5.451918\n",
            "epoch: 83300 D_Loss = 0.06904337 acc: 50.0 G_Loss = 0.1940797\n",
            "epoch: 83350 D_Loss = 0.10310191 acc: 50.0 G_Loss = 0.069636256\n",
            "epoch: 83400 D_Loss = 0.09114358 acc: 50.0 G_Loss = 0.030105924\n",
            "epoch: 83450 D_Loss = 0.15298901 acc: 50.0 G_Loss = 0.25299\n",
            "epoch: 83500 D_Loss = 0.18386829 acc: 50.0 G_Loss = 0.16993923\n",
            "epoch: 83550 D_Loss = 0.14883842 acc: 50.0 G_Loss = 0.3618069\n",
            "epoch: 83600 D_Loss = 0.019476993 acc: 50.0 G_Loss = 0.16755538\n",
            "epoch: 83650 D_Loss = 0.19427228 acc: 50.0 G_Loss = 2.1202555\n",
            "epoch: 83700 D_Loss = 0.26605034 acc: 46.875 G_Loss = 3.3322694\n",
            "epoch: 83750 D_Loss = 0.15487652 acc: 50.0 G_Loss = 0.06355146\n",
            "epoch: 83800 D_Loss = 0.15795743 acc: 50.0 G_Loss = 0.03452518\n",
            "epoch: 83850 D_Loss = 0.15686578 acc: 50.0 G_Loss = 0.033357643\n",
            "epoch: 83900 D_Loss = 0.1522724 acc: 50.0 G_Loss = 0.21518755\n",
            "epoch: 83950 D_Loss = 0.10551148 acc: 50.0 G_Loss = 0.26459476\n",
            "epoch: 84000 D_Loss = 0.17560063 acc: 50.0 G_Loss = 0.2811967\n",
            "epoch: 84050 D_Loss = 0.18477292 acc: 50.0 G_Loss = 0.64644283\n",
            "epoch: 84100 D_Loss = 0.086065404 acc: 50.0 G_Loss = 0.26004636\n",
            "epoch: 84150 D_Loss = 0.04233648 acc: 50.0 G_Loss = 0.07986815\n",
            "epoch: 84200 D_Loss = 0.15424125 acc: 50.0 G_Loss = 0.064749785\n",
            "epoch: 84250 D_Loss = 0.17308769 acc: 50.0 G_Loss = 0.035237305\n",
            "epoch: 84300 D_Loss = 0.07327953 acc: 50.0 G_Loss = 0.96368253\n",
            "epoch: 84350 D_Loss = 0.21363568 acc: 50.0 G_Loss = 0.28371453\n",
            "epoch: 84400 D_Loss = 0.105753385 acc: 50.0 G_Loss = 0.75479764\n",
            "epoch: 84450 D_Loss = 0.1504847 acc: 50.0 G_Loss = 0.08832402\n",
            "epoch: 84500 D_Loss = 0.1799718 acc: 50.0 G_Loss = 0.056830212\n",
            "epoch: 84550 D_Loss = 0.16152954 acc: 50.0 G_Loss = 0.70937794\n",
            "epoch: 84600 D_Loss = 0.11875376 acc: 50.0 G_Loss = 1.9362941\n",
            "epoch: 84650 D_Loss = 0.09154398 acc: 50.0 G_Loss = 0.41759333\n",
            "epoch: 84700 D_Loss = 0.124538444 acc: 50.0 G_Loss = 0.11518542\n",
            "epoch: 84750 D_Loss = 0.16528575 acc: 50.0 G_Loss = 0.10122666\n",
            "epoch: 84800 D_Loss = 0.13120876 acc: 50.0 G_Loss = 0.16621208\n",
            "epoch: 84850 D_Loss = 0.16506445 acc: 50.0 G_Loss = 0.43189013\n",
            "epoch: 84900 D_Loss = 0.1698991 acc: 50.0 G_Loss = 0.29845393\n",
            "epoch: 84950 D_Loss = 0.13922213 acc: 50.0 G_Loss = 0.14752436\n",
            "epoch: 85000 D_Loss = 0.13705532 acc: 50.0 G_Loss = 0.056416307\n",
            "epoch: 85050 D_Loss = 0.15960291 acc: 50.0 G_Loss = 0.052391615\n",
            "epoch: 85100 D_Loss = 0.06284913 acc: 50.0 G_Loss = 0.014798835\n",
            "epoch: 85150 D_Loss = 0.17603596 acc: 50.0 G_Loss = 0.1281051\n",
            "epoch: 85200 D_Loss = 0.0565194 acc: 50.0 G_Loss = 0.13615978\n",
            "epoch: 85250 D_Loss = 0.110899694 acc: 50.0 G_Loss = 0.085057944\n",
            "epoch: 85300 D_Loss = 0.054723885 acc: 50.0 G_Loss = 0.15144098\n",
            "epoch: 85350 D_Loss = 0.06558695 acc: 50.0 G_Loss = 0.066500604\n",
            "epoch: 85400 D_Loss = 0.16703454 acc: 50.0 G_Loss = 0.38727546\n",
            "epoch: 85450 D_Loss = 0.1140955 acc: 50.0 G_Loss = 0.1473869\n",
            "epoch: 85500 D_Loss = 0.18354638 acc: 50.0 G_Loss = 0.11374745\n",
            "epoch: 85550 D_Loss = 0.12369691 acc: 50.0 G_Loss = 0.76607823\n",
            "epoch: 85600 D_Loss = 0.18097182 acc: 50.0 G_Loss = 0.10214331\n",
            "epoch: 85650 D_Loss = 0.113559216 acc: 50.0 G_Loss = 0.0836101\n",
            "epoch: 85700 D_Loss = 0.08795477 acc: 50.0 G_Loss = 0.09562401\n",
            "epoch: 85750 D_Loss = 0.108604215 acc: 50.0 G_Loss = 0.6353627\n",
            "epoch: 85800 D_Loss = 0.17338352 acc: 50.0 G_Loss = 0.20597473\n",
            "epoch: 85850 D_Loss = 0.14013335 acc: 50.0 G_Loss = 0.12361841\n",
            "epoch: 85900 D_Loss = 0.21140628 acc: 50.0 G_Loss = 0.2818427\n",
            "epoch: 85950 D_Loss = 0.18619043 acc: 50.0 G_Loss = 0.5189188\n",
            "epoch: 86000 D_Loss = 0.15338519 acc: 50.0 G_Loss = 3.3499658\n",
            "epoch: 86050 D_Loss = 0.1300322 acc: 50.0 G_Loss = 0.04803887\n",
            "epoch: 86100 D_Loss = 0.1928391 acc: 50.0 G_Loss = 0.6636037\n",
            "epoch: 86150 D_Loss = 0.0604035 acc: 50.0 G_Loss = 0.03246013\n",
            "epoch: 86200 D_Loss = 0.032360855 acc: 50.0 G_Loss = 0.10053009\n",
            "epoch: 86250 D_Loss = 0.1198237 acc: 50.0 G_Loss = 0.65464157\n",
            "epoch: 86300 D_Loss = 0.15883106 acc: 50.0 G_Loss = 2.0942879\n",
            "epoch: 86350 D_Loss = 0.12137198 acc: 50.0 G_Loss = 0.16630086\n",
            "epoch: 86400 D_Loss = 0.00842066 acc: 50.0 G_Loss = 0.28057227\n",
            "epoch: 86450 D_Loss = 0.070590764 acc: 50.0 G_Loss = 0.36411613\n",
            "epoch: 86500 D_Loss = 0.17077245 acc: 50.0 G_Loss = 0.32470408\n",
            "epoch: 86550 D_Loss = 0.15151736 acc: 50.0 G_Loss = 0.21801601\n",
            "epoch: 86600 D_Loss = 0.054303158 acc: 50.0 G_Loss = 0.063035615\n",
            "epoch: 86650 D_Loss = 0.20610479 acc: 50.0 G_Loss = 0.30623537\n",
            "epoch: 86700 D_Loss = 0.07256526 acc: 50.0 G_Loss = 0.110915236\n",
            "epoch: 86750 D_Loss = 0.12540564 acc: 50.0 G_Loss = 0.18825477\n",
            "epoch: 86800 D_Loss = 0.16109629 acc: 50.0 G_Loss = 3.1779892\n",
            "epoch: 86850 D_Loss = 0.19725986 acc: 50.0 G_Loss = 0.5768385\n",
            "epoch: 86900 D_Loss = 0.121567406 acc: 50.0 G_Loss = 0.06957187\n",
            "epoch: 86950 D_Loss = 0.07826924 acc: 50.0 G_Loss = 0.027281815\n",
            "epoch: 87000 D_Loss = 0.19216214 acc: 50.0 G_Loss = 0.06610696\n",
            "epoch: 87050 D_Loss = 0.20603046 acc: 50.0 G_Loss = 4.070162\n",
            "epoch: 87100 D_Loss = 0.16012421 acc: 50.0 G_Loss = 0.10541363\n",
            "epoch: 87150 D_Loss = 0.17168435 acc: 50.0 G_Loss = 0.99874365\n",
            "epoch: 87200 D_Loss = 0.0961158 acc: 50.0 G_Loss = 3.0433817\n",
            "epoch: 87250 D_Loss = 0.22336 acc: 50.0 G_Loss = 0.020791844\n",
            "epoch: 87300 D_Loss = 0.1384627 acc: 50.0 G_Loss = 0.0814393\n",
            "epoch: 87350 D_Loss = 0.068972625 acc: 50.0 G_Loss = 0.06031503\n",
            "epoch: 87400 D_Loss = 0.14598908 acc: 50.0 G_Loss = 0.16670172\n",
            "epoch: 87450 D_Loss = 0.10223695 acc: 50.0 G_Loss = 4.4709425\n",
            "epoch: 87500 D_Loss = 0.17622812 acc: 50.0 G_Loss = 0.41191703\n",
            "epoch: 87550 D_Loss = 0.04547595 acc: 50.0 G_Loss = 0.810692\n",
            "epoch: 87600 D_Loss = 0.104487546 acc: 50.0 G_Loss = 0.05190119\n",
            "epoch: 87650 D_Loss = 0.19543602 acc: 50.0 G_Loss = 0.13219571\n",
            "epoch: 87700 D_Loss = 0.035255056 acc: 50.0 G_Loss = 0.12419213\n",
            "epoch: 87750 D_Loss = 0.11335033 acc: 50.0 G_Loss = 0.08479151\n",
            "epoch: 87800 D_Loss = 0.076286666 acc: 50.0 G_Loss = 0.29400063\n",
            "epoch: 87850 D_Loss = 0.1014164 acc: 50.0 G_Loss = 0.1441925\n",
            "epoch: 87900 D_Loss = 0.11808577 acc: 50.0 G_Loss = 0.14732592\n",
            "epoch: 87950 D_Loss = 0.17414315 acc: 50.0 G_Loss = 0.2584523\n",
            "epoch: 88000 D_Loss = 0.08163647 acc: 50.0 G_Loss = 0.0875883\n",
            "epoch: 88050 D_Loss = 0.06761185 acc: 50.0 G_Loss = 0.055906076\n",
            "epoch: 88100 D_Loss = 0.059087187 acc: 50.0 G_Loss = 0.121546336\n",
            "epoch: 88150 D_Loss = 0.13789342 acc: 50.0 G_Loss = 0.9109798\n",
            "epoch: 88200 D_Loss = 0.09671811 acc: 50.0 G_Loss = 1.3066578\n",
            "epoch: 88250 D_Loss = 0.037958253 acc: 50.0 G_Loss = 0.036877353\n",
            "epoch: 88300 D_Loss = 0.108320355 acc: 50.0 G_Loss = 0.008793667\n",
            "epoch: 88350 D_Loss = 0.117881835 acc: 50.0 G_Loss = 0.026773194\n",
            "epoch: 88400 D_Loss = 0.089825675 acc: 50.0 G_Loss = 0.024577498\n",
            "epoch: 88450 D_Loss = 0.056482203 acc: 50.0 G_Loss = 0.26340407\n",
            "epoch: 88500 D_Loss = 0.09438231 acc: 50.0 G_Loss = 0.13235353\n",
            "epoch: 88550 D_Loss = 0.12252091 acc: 50.0 G_Loss = 0.12174744\n",
            "epoch: 88600 D_Loss = 0.11809789 acc: 50.0 G_Loss = 0.2164128\n",
            "epoch: 88650 D_Loss = 0.2029212 acc: 50.0 G_Loss = 0.23498185\n",
            "epoch: 88700 D_Loss = 0.11273085 acc: 50.0 G_Loss = 1.9081628\n",
            "epoch: 88750 D_Loss = 0.022941886 acc: 50.0 G_Loss = 0.069947496\n",
            "epoch: 88800 D_Loss = 0.16076227 acc: 50.0 G_Loss = 0.21613607\n",
            "epoch: 88850 D_Loss = 0.17740089 acc: 50.0 G_Loss = 0.46254748\n",
            "epoch: 88900 D_Loss = 0.15721934 acc: 50.0 G_Loss = 0.22626953\n",
            "epoch: 88950 D_Loss = 0.05210581 acc: 50.0 G_Loss = 0.058883753\n",
            "epoch: 89000 D_Loss = 0.21306759 acc: 50.0 G_Loss = 0.03617956\n",
            "epoch: 89050 D_Loss = 0.16263276 acc: 50.0 G_Loss = 0.07463055\n",
            "epoch: 89100 D_Loss = 0.07805416 acc: 50.0 G_Loss = 0.20552433\n",
            "epoch: 89150 D_Loss = 0.027065996 acc: 50.0 G_Loss = 0.2678647\n",
            "epoch: 89200 D_Loss = 0.2234781 acc: 50.0 G_Loss = 0.6184319\n",
            "epoch: 89250 D_Loss = 0.03713992 acc: 50.0 G_Loss = 0.056609966\n",
            "epoch: 89300 D_Loss = 0.048768472 acc: 50.0 G_Loss = 0.018795125\n",
            "epoch: 89350 D_Loss = 0.03702075 acc: 50.0 G_Loss = 0.019321952\n",
            "epoch: 89400 D_Loss = 0.12739505 acc: 50.0 G_Loss = 0.3035938\n",
            "epoch: 89450 D_Loss = 0.17762654 acc: 50.0 G_Loss = 1.3231034\n",
            "epoch: 89500 D_Loss = 0.18280521 acc: 50.0 G_Loss = 0.38439685\n",
            "epoch: 89550 D_Loss = 0.14931507 acc: 50.0 G_Loss = 0.09689127\n",
            "epoch: 89600 D_Loss = 0.19521901 acc: 50.0 G_Loss = 0.17385931\n",
            "epoch: 89650 D_Loss = 0.123350404 acc: 50.0 G_Loss = 0.03955595\n",
            "epoch: 89700 D_Loss = 0.117190145 acc: 50.0 G_Loss = 0.15610273\n",
            "epoch: 89750 D_Loss = 0.14058936 acc: 50.0 G_Loss = 0.023373943\n",
            "epoch: 89800 D_Loss = 0.065791704 acc: 50.0 G_Loss = 0.23539795\n",
            "epoch: 89850 D_Loss = 0.11703821 acc: 50.0 G_Loss = 0.13634707\n",
            "epoch: 89900 D_Loss = 0.11984977 acc: 50.0 G_Loss = 1.1258875\n",
            "epoch: 89950 D_Loss = 0.029088194 acc: 50.0 G_Loss = 0.019831883\n",
            "epoch: 90000 D_Loss = 0.018238788 acc: 50.0 G_Loss = 0.016866172\n",
            "epoch: 90050 D_Loss = 0.03691667 acc: 50.0 G_Loss = 0.021903805\n",
            "epoch: 90100 D_Loss = 0.054523684 acc: 50.0 G_Loss = 0.019196657\n",
            "epoch: 90150 D_Loss = 0.16369662 acc: 50.0 G_Loss = 0.13390648\n",
            "epoch: 90200 D_Loss = 0.09524656 acc: 50.0 G_Loss = 0.23027948\n",
            "epoch: 90250 D_Loss = 0.17578167 acc: 50.0 G_Loss = 1.0894923\n",
            "epoch: 90300 D_Loss = 0.17671727 acc: 50.0 G_Loss = 0.018230235\n",
            "epoch: 90350 D_Loss = 0.14912271 acc: 50.0 G_Loss = 0.017747484\n",
            "epoch: 90400 D_Loss = 0.1657046 acc: 50.0 G_Loss = 0.069304034\n",
            "epoch: 90450 D_Loss = 0.14887771 acc: 50.0 G_Loss = 0.09070471\n",
            "epoch: 90500 D_Loss = 0.13268083 acc: 50.0 G_Loss = 0.16631699\n",
            "epoch: 90550 D_Loss = 0.10036375 acc: 50.0 G_Loss = 0.014582016\n",
            "epoch: 90600 D_Loss = 0.11235981 acc: 50.0 G_Loss = 0.075821854\n",
            "epoch: 90650 D_Loss = 0.16284463 acc: 50.0 G_Loss = 2.5282712\n",
            "epoch: 90700 D_Loss = 0.17114922 acc: 50.0 G_Loss = 0.16324046\n",
            "epoch: 90750 D_Loss = 0.11882034 acc: 50.0 G_Loss = 0.08697742\n",
            "epoch: 90800 D_Loss = 0.15337431 acc: 50.0 G_Loss = 0.05034571\n",
            "epoch: 90850 D_Loss = 0.17503847 acc: 50.0 G_Loss = 0.09090693\n",
            "epoch: 90900 D_Loss = 0.15903445 acc: 50.0 G_Loss = 0.5627434\n",
            "epoch: 90950 D_Loss = 0.120902576 acc: 50.0 G_Loss = 0.25183567\n",
            "epoch: 91000 D_Loss = 0.16967562 acc: 50.0 G_Loss = 0.09898283\n",
            "epoch: 91050 D_Loss = 0.04085617 acc: 50.0 G_Loss = 0.47573632\n",
            "epoch: 91100 D_Loss = 0.084880896 acc: 50.0 G_Loss = 0.05961656\n",
            "epoch: 91150 D_Loss = 0.1561017 acc: 50.0 G_Loss = 0.10776934\n",
            "epoch: 91200 D_Loss = 0.07842928 acc: 50.0 G_Loss = 0.108679086\n",
            "epoch: 91250 D_Loss = 0.07785644 acc: 50.0 G_Loss = 0.02109196\n",
            "epoch: 91300 D_Loss = 0.046067275 acc: 50.0 G_Loss = 0.09036595\n",
            "epoch: 91350 D_Loss = 0.105436295 acc: 50.0 G_Loss = 0.46005273\n",
            "epoch: 91400 D_Loss = 0.11203847 acc: 50.0 G_Loss = 0.48999727\n",
            "epoch: 91450 D_Loss = 0.11353396 acc: 50.0 G_Loss = 0.1983425\n",
            "epoch: 91500 D_Loss = 0.07605428 acc: 50.0 G_Loss = 0.085170835\n",
            "epoch: 91550 D_Loss = 0.117497504 acc: 50.0 G_Loss = 0.07992204\n",
            "epoch: 91600 D_Loss = 0.11313835 acc: 50.0 G_Loss = 0.021468364\n",
            "epoch: 91650 D_Loss = 0.17820585 acc: 50.0 G_Loss = 0.16705108\n",
            "epoch: 91700 D_Loss = 0.07943537 acc: 50.0 G_Loss = 0.08991223\n",
            "epoch: 91750 D_Loss = 0.09886623 acc: 50.0 G_Loss = 3.1988025\n",
            "epoch: 91800 D_Loss = 0.06458801 acc: 50.0 G_Loss = 0.14834389\n",
            "epoch: 91850 D_Loss = 0.0702191 acc: 50.0 G_Loss = 0.2683962\n",
            "epoch: 91900 D_Loss = 0.1837379 acc: 50.0 G_Loss = 0.89522284\n",
            "epoch: 91950 D_Loss = 0.07900446 acc: 50.0 G_Loss = 0.049268328\n",
            "epoch: 92000 D_Loss = 0.14591275 acc: 50.0 G_Loss = 0.10183826\n",
            "epoch: 92050 D_Loss = 0.17774607 acc: 50.0 G_Loss = 0.02013593\n",
            "epoch: 92100 D_Loss = 0.050298296 acc: 50.0 G_Loss = 0.02810031\n",
            "epoch: 92150 D_Loss = 0.058503598 acc: 50.0 G_Loss = 1.191683\n",
            "epoch: 92200 D_Loss = 0.08685031 acc: 50.0 G_Loss = 0.36654136\n",
            "epoch: 92250 D_Loss = 0.15988429 acc: 50.0 G_Loss = 0.17920661\n",
            "epoch: 92300 D_Loss = 0.1746648 acc: 50.0 G_Loss = 0.124366246\n",
            "epoch: 92350 D_Loss = 0.06990042 acc: 50.0 G_Loss = 0.04711599\n",
            "epoch: 92400 D_Loss = 0.1043513 acc: 50.0 G_Loss = 0.2963217\n",
            "epoch: 92450 D_Loss = 0.06094696 acc: 50.0 G_Loss = 0.32291573\n",
            "epoch: 92500 D_Loss = 0.11737145 acc: 50.0 G_Loss = 0.21304113\n",
            "epoch: 92550 D_Loss = 0.14697923 acc: 50.0 G_Loss = 0.43191254\n",
            "epoch: 92600 D_Loss = 0.11755078 acc: 50.0 G_Loss = 0.06573725\n",
            "epoch: 92650 D_Loss = 0.17033073 acc: 50.0 G_Loss = 0.1227287\n",
            "epoch: 92700 D_Loss = 0.052184477 acc: 50.0 G_Loss = 0.03825822\n",
            "epoch: 92750 D_Loss = 0.1298029 acc: 50.0 G_Loss = 0.3439005\n",
            "epoch: 92800 D_Loss = 0.07660318 acc: 50.0 G_Loss = 0.31925988\n",
            "epoch: 92850 D_Loss = 0.058670864 acc: 50.0 G_Loss = 0.12746\n",
            "epoch: 92900 D_Loss = 0.049264275 acc: 50.0 G_Loss = 2.614115\n",
            "epoch: 92950 D_Loss = 0.08991773 acc: 50.0 G_Loss = 0.18915607\n",
            "epoch: 93000 D_Loss = 0.14445984 acc: 50.0 G_Loss = 0.68927157\n",
            "epoch: 93050 D_Loss = 0.17927319 acc: 50.0 G_Loss = 0.22918822\n",
            "epoch: 93100 D_Loss = 0.08768404 acc: 50.0 G_Loss = 0.1256815\n",
            "epoch: 93150 D_Loss = 0.081918664 acc: 50.0 G_Loss = 0.12580171\n",
            "epoch: 93200 D_Loss = 0.06384259 acc: 50.0 G_Loss = 0.31012788\n",
            "epoch: 93250 D_Loss = 0.041919533 acc: 50.0 G_Loss = 0.023229359\n",
            "epoch: 93300 D_Loss = 0.039602913 acc: 50.0 G_Loss = 0.104215756\n",
            "epoch: 93350 D_Loss = 0.16263525 acc: 50.0 G_Loss = 0.17084165\n",
            "epoch: 93400 D_Loss = 0.09222324 acc: 50.0 G_Loss = 0.031765074\n",
            "epoch: 93450 D_Loss = 0.04981305 acc: 50.0 G_Loss = 0.09737532\n",
            "epoch: 93500 D_Loss = 0.12355187 acc: 50.0 G_Loss = 1.01702\n",
            "epoch: 93550 D_Loss = 0.18132256 acc: 50.0 G_Loss = 0.1907003\n",
            "epoch: 93600 D_Loss = 0.13201518 acc: 50.0 G_Loss = 2.7397234\n",
            "epoch: 93650 D_Loss = 0.082872085 acc: 50.0 G_Loss = 0.107082956\n",
            "epoch: 93700 D_Loss = 0.13621958 acc: 50.0 G_Loss = 0.13467225\n",
            "epoch: 93750 D_Loss = 0.070452176 acc: 50.0 G_Loss = 0.9524817\n",
            "epoch: 93800 D_Loss = 0.09940426 acc: 50.0 G_Loss = 0.20339218\n",
            "epoch: 93850 D_Loss = 0.13586895 acc: 50.0 G_Loss = 0.16321325\n",
            "epoch: 93900 D_Loss = 0.11849221 acc: 50.0 G_Loss = 0.25252834\n",
            "epoch: 93950 D_Loss = 0.12728283 acc: 50.0 G_Loss = 0.4620601\n",
            "epoch: 94000 D_Loss = 0.08443023 acc: 50.0 G_Loss = 0.0514364\n",
            "epoch: 94050 D_Loss = 0.124751784 acc: 50.0 G_Loss = 0.11127205\n",
            "epoch: 94100 D_Loss = 0.20794392 acc: 49.21875 G_Loss = 7.560721\n",
            "epoch: 94150 D_Loss = 0.13693808 acc: 50.0 G_Loss = 0.2807875\n",
            "epoch: 94200 D_Loss = 0.15055677 acc: 50.0 G_Loss = 0.3646817\n",
            "epoch: 94250 D_Loss = 0.12552945 acc: 50.0 G_Loss = 0.14430824\n",
            "epoch: 94300 D_Loss = 0.12631229 acc: 50.0 G_Loss = 0.10220751\n",
            "epoch: 94350 D_Loss = 0.08501105 acc: 50.0 G_Loss = 0.12678537\n",
            "epoch: 94400 D_Loss = 0.124789394 acc: 50.0 G_Loss = 0.1300517\n",
            "epoch: 94450 D_Loss = 0.0933199 acc: 50.0 G_Loss = 0.0394768\n",
            "epoch: 94500 D_Loss = 0.1532317 acc: 50.0 G_Loss = 0.32204187\n",
            "epoch: 94550 D_Loss = 0.11729455 acc: 50.0 G_Loss = 0.20983669\n",
            "epoch: 94600 D_Loss = 0.15129715 acc: 50.0 G_Loss = 1.1940792\n",
            "epoch: 94650 D_Loss = 0.07282595 acc: 50.0 G_Loss = 0.061348595\n",
            "epoch: 94700 D_Loss = 0.085731454 acc: 50.0 G_Loss = 0.042016752\n",
            "epoch: 94750 D_Loss = 0.031205878 acc: 50.0 G_Loss = 0.07388784\n",
            "epoch: 94800 D_Loss = 0.050804045 acc: 50.0 G_Loss = 0.07658756\n",
            "epoch: 94850 D_Loss = 0.1328075 acc: 50.0 G_Loss = 0.15926659\n",
            "epoch: 94900 D_Loss = 0.16867971 acc: 50.0 G_Loss = 2.963469\n",
            "epoch: 94950 D_Loss = 0.22095013 acc: 50.0 G_Loss = 2.6503432\n",
            "epoch: 95000 D_Loss = 0.14057648 acc: 50.0 G_Loss = 0.1305777\n",
            "epoch: 95050 D_Loss = 0.18754224 acc: 50.0 G_Loss = 0.22063604\n",
            "epoch: 95100 D_Loss = 0.052741636 acc: 50.0 G_Loss = 0.021994814\n",
            "epoch: 95150 D_Loss = 0.1379 acc: 50.0 G_Loss = 0.33382654\n",
            "epoch: 95200 D_Loss = 0.10950685 acc: 50.0 G_Loss = 0.08633739\n",
            "epoch: 95250 D_Loss = 0.17290531 acc: 50.0 G_Loss = 0.47078007\n",
            "epoch: 95300 D_Loss = 0.020429509 acc: 50.0 G_Loss = 0.05442048\n",
            "epoch: 95350 D_Loss = 0.15947297 acc: 50.0 G_Loss = 0.4737605\n",
            "epoch: 95400 D_Loss = 0.09149339 acc: 50.0 G_Loss = 3.513045\n",
            "epoch: 95450 D_Loss = 0.19687013 acc: 50.0 G_Loss = 0.33039358\n",
            "epoch: 95500 D_Loss = 0.08158711 acc: 50.0 G_Loss = 0.062103745\n",
            "epoch: 95550 D_Loss = 0.09663127 acc: 50.0 G_Loss = 0.27814156\n",
            "epoch: 95600 D_Loss = 0.07443441 acc: 49.21875 G_Loss = 0.03845979\n",
            "epoch: 95650 D_Loss = 0.10081611 acc: 50.0 G_Loss = 0.14769378\n",
            "epoch: 95700 D_Loss = 0.1979934 acc: 50.0 G_Loss = 3.653963\n",
            "epoch: 95750 D_Loss = 0.12622395 acc: 50.0 G_Loss = 0.3337927\n",
            "epoch: 95800 D_Loss = 0.09370306 acc: 50.0 G_Loss = 0.15469892\n",
            "epoch: 95850 D_Loss = 0.08278725 acc: 50.0 G_Loss = 0.11377666\n",
            "epoch: 95900 D_Loss = 0.12450689 acc: 50.0 G_Loss = 0.089116156\n",
            "epoch: 95950 D_Loss = 0.103948236 acc: 50.0 G_Loss = 0.0673099\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p85i6Zpz7f8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator.save_weights('generator.h5')\n",
        "discriminator.save_weights('discriminator.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBdT1NgAKigt",
        "colab_type": "text"
      },
      "source": [
        "# Generating Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFYdbZV6KjV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_imgs(100000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPOiUz29Knd3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "noise = np.random.normal(0, 1, (1, noise_shape))\n",
        "gen_imgs = generator.predict(noise)\n",
        "gen_imgs = gen_imgs*(1/2.5) + 0.5\n",
        "\n",
        "plt.imshow(gen_imgs.reshape((128 , 128, 3)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ1HiS37Ko-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator.save_weights('generator.h5')\n",
        "discriminator.save_weights('discriminator.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}