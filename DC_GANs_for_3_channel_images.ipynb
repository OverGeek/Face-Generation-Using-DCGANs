{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DC_GANs_for_3_channel_images.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLx4oMnH2StW",
        "colab_type": "text"
      },
      "source": [
        "# Downloading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRcpwKBgIA1M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3\" --header=\"Accept-Language: en-US,en;q=0.9\" --header=\"Referer: https://www.kaggle.com/\" \"https://storage.googleapis.com/kaggle-datasets/122892/296485/celebrities-100k.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1559398353&Signature=KOf7hc8aAQ71VzYxa7okNbGcOD8JnLF2pLj6ZKsO4o5aCLwY1COiFQ5nwWvKa%2ByXh%2BV39Xmm3ew4R8ZtsaVS4HfFbHyEpagAXWfm7GxPs9S3qMgKYByDJoR9uPaSqmakPDAhGBrGT5GZwE7YDIR2G5%2BbKrVC%2BcbWg9PJNDYE%2B%2BJiMZlGv%2BuAGo0%2BFkJjF%2FB5iXGJonlqljkTiRQJ8R89Ru0dz6LO1tKc8TrvSOEGBGAsNMV4GFoNbPzLssmAM7Rg1XGP271Q9rRqRuXhGRYDwEEHOdQNXlceSsLooi3tl%2B%2FpaPuKisk7vQ%2BUc0nC%2FuekVw4hPYqcqP4HE8%2BkG0NwGg%3D%3D\" -O \"celebrities-100k.zip\" -c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peLSfzZ50cKc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip 'celebrities-100k.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzWjbAyF8JfI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip '100k.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-bS75jG2XwW",
        "colab_type": "text"
      },
      "source": [
        "# Building Model and Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuu6baRTVgmR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir 'Model Architecture'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS-Fe5h42Php",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "from keras.layers import Dense, Dropout, Flatten, Reshape, Input, BatchNormalization, Activation, ZeroPadding2D, MaxPooling2D, AveragePooling2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Model, Sequential\n",
        "from keras.optimizers import Adam\n",
        "from PIL import Image\n",
        "from keras.utils import plot_model\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sys\n",
        "import os as os\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJdEXvLC2kzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "noise_shape = 100\n",
        "nh = 64\n",
        "nw = 64\n",
        "nc = 3\n",
        "img_shape = (nh, nw, nc)\n",
        "\n",
        "# Carefully chosen parameters \n",
        "d_opt = Adam(lr = 0.00004, beta_1 = 0.5)\n",
        "g_opt = Adam(lr = 0.0002, beta_1 = 0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeyQN08j3F1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_generator():\n",
        "  \n",
        "  model = Sequential()\n",
        "  model.add(Dense(128 * 4 * 4, activation=\"relu\", input_dim = noise_shape, name = 'Dense_Layer'))\n",
        "  model.add(Reshape((4, 4, 128) , name = \"Reshape_Layer\"))\n",
        "  \n",
        "  model.add(UpSampling2D(name = 'UpSampling_Layer'))\n",
        "  model.add(Conv2D(64, kernel_size=3,  padding=\"same\", name = \"Conv_Layer\"))\n",
        "  model.add(LeakyReLU(alpha=0.2, name = 'Leaky_ReLU'))\n",
        "  model.add(BatchNormalization(momentum=0.8, name = \"Batch_Norm\"))\n",
        "  \n",
        "  model.add(UpSampling2D(name = 'UpSampling_Layer_2'))\n",
        "  model.add(Conv2D(32, kernel_size=3, padding=\"same\", name = 'Conv_Layer_2'))\n",
        "  model.add(LeakyReLU(alpha=0.2, name = 'Leaky_ReLU_2'))\n",
        "  model.add(BatchNormalization(momentum=0.8, name = 'Batch_Norm_2'))\n",
        "  \n",
        "  model.add(UpSampling2D(name = 'UpSampling_Layer_3'))\n",
        "  model.add(Conv2D(16, kernel_size=3, padding=\"same\", name = 'Conv_Layer_3'))\n",
        "  model.add(LeakyReLU(alpha=0.2, name = 'Leaky_ReLU_3'))\n",
        "  model.add(BatchNormalization(momentum=0.8, name = 'Batch_Norm_3'))\n",
        "  \n",
        "  model.add(UpSampling2D(name = 'UpSampling_Layer_4'))\n",
        "  model.add(Conv2D(8, kernel_size=3, padding=\"same\", name = 'Conv_Layer_4'))\n",
        "  model.add(LeakyReLU(alpha=0.2, name = 'Leaky_ReLU_4'))\n",
        "  model.add(BatchNormalization(momentum=0.8, name = 'Batch_Norm_4'))\n",
        "  \n",
        "  model.add(Conv2D(4, kernel_size=3, padding=\"same\", name = 'Conv_Layer_5'))\n",
        "  model.add(LeakyReLU(alpha=0.2, name = 'Leaky_ReLU_5'))\n",
        "  model.add(BatchNormalization(momentum=0.8, name = 'Batch_Norm_5'))\n",
        "  \n",
        "  model.add(Conv2D(3, kernel_size=3, padding=\"same\", name = 'Conv_Layer_6'))\n",
        "  model.add(LeakyReLU(alpha=0.2, name = 'Leaky_ReLU_6'))\n",
        "  model.add(BatchNormalization(momentum=0.8, name = 'Batch_Norm_6'))\n",
        "  model.add(Activation(\"tanh\"))\n",
        "   \n",
        "  print(model.summary())\n",
        "  \n",
        "  #Picturizing model\n",
        "  plot_model(model, to_file='Model Architecture/generator.png')\n",
        "  \n",
        "  noise = Input(shape = (noise_shape,))\n",
        "  img = model(noise)\n",
        "  \n",
        "  return Model(inputs = noise, outputs = img, name = \"Generator\")\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6QaylpG3swT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator = build_generator()\n",
        "generator.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwqnDYfm3uXu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_discriminator():\n",
        "  model = Sequential()\n",
        "  \n",
        "  model.add(Conv2D(4, kernel_size=3, input_shape = img_shape, padding=\"same\", name = 'Conv_Layer'))\n",
        "  model.add(LeakyReLU(alpha=0.2, name = 'Leaky_ReLU'))\n",
        "  model.add(BatchNormalization(momentum=0.8, name = 'Batch_Norm'))\n",
        "  \n",
        "  model.add(Conv2D(8, kernel_size=3, strides = 2, name = 'Conv_Layer_2'))\n",
        "  model.add(LeakyReLU(alpha=0.2, name = 'Leaky_ReLU_2'))\n",
        "  model.add(BatchNormalization(momentum=0.8, name = 'Batch_Norm_2'))\n",
        "  \n",
        "  model.add(Conv2D(16, kernel_size=3, strides = 2, name = 'Conv_Layer_3'))\n",
        "  model.add(LeakyReLU(alpha=0.2, name = 'Leaky_ReLU_3'))\n",
        "  model.add(BatchNormalization(momentum=0.8, name = 'Batch_Norm_3'))\n",
        "  \n",
        "  model.add(Conv2D(32, kernel_size=3, strides = 2, name = 'Conv_Layer_4'))\n",
        "  model.add(LeakyReLU(alpha=0.2, name = 'Leaky_ReLU_4'))\n",
        "  model.add(BatchNormalization(momentum=0.8, name = 'Batch_Norm_4'))\n",
        "  \n",
        "  model.add(Conv2D(64, kernel_size=2, strides = 1, name = 'Conv_Layer_5'))\n",
        "  model.add(LeakyReLU(alpha=0.2, name = 'Leaky_ReLU_5'))\n",
        "  model.add(BatchNormalization(momentum=0.8, name = 'Batch_Norm_5'))\n",
        "  \n",
        "  \n",
        "  model.add(Flatten(name = 'Flatten'))\n",
        "  model.add(Dense(512, name = 'Dense_Layer'))\n",
        "  model.add(LeakyReLU(alpha=0.2, name = 'Leaky_ReLU_6'))\n",
        "  \n",
        "  model.add(Dense(256, name = 'Dense_Layer_2'))\n",
        "  model.add(LeakyReLU(alpha=0.2, name = 'Leaky_ReLU_7'))\n",
        "  model.add(Dense(1, activation='sigmoid', name = 'Dense_with_Sigmoid'))\n",
        "  \n",
        "  print(model.summary())\n",
        "  \n",
        "  #Picturizing model\n",
        "  plot_model(model, to_file='Model Architecture/discriminator.png')\n",
        "  \n",
        "  img = Input(shape = img_shape)\n",
        "  validity = model(img)\n",
        "  \n",
        "  return Model(inputs = img, outputs = validity, name = 'Discriminator')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwqJneJS5Wpi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discriminator = build_discriminator()\n",
        "discriminator.summary()\n",
        "\n",
        "discriminator.compile(loss = 'binary_crossentropy', optimizer = d_opt, metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8NZedcg5W4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z = Input(shape = (noise_shape,), name = \"Input_Noise\")\n",
        "img = generator(z)\n",
        "\n",
        "discriminator.trainable = False\n",
        "valid = discriminator(img)\n",
        "\n",
        "combined = Model(z, valid)\n",
        "combined.compile(loss = 'binary_crossentropy', optimizer = g_opt)\n",
        "combined.summary()\n",
        "\n",
        "#Picturizing model\n",
        "plot_model(combined, to_file='Model Architecture/combined.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpHkFPDy5cTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(generator.summary())\n",
        "print(discriminator.summary())\n",
        "print(combined.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiYeYnjHIdQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batch(width, height):\n",
        "  image_files = os.listdir('/content/100k/')[:]\n",
        "  data_batch = []\n",
        "  \n",
        "  for file in image_files:\n",
        "    img = Image.open('/content/100k/'+file).resize([width, height])\n",
        "    data_batch.append(np.array(img.convert('RGB')))\n",
        "    \n",
        "  data_batch = np.array(data_batch)\n",
        "  print(data_batch.shape)\n",
        "  return data_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYiMGq1g5dwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(batch_size = 64, epochs = 4000):\n",
        "  \n",
        "  \n",
        "  valid = np.ones((batch_size, 1))\n",
        "  fakes = np.zeros((batch_size, 1))\n",
        "  \n",
        "  X_train = get_batch(nw, nh)\n",
        "  X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
        "  \n",
        "  for epoch in range(epochs):\n",
        "    \n",
        "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "    imgs = X_train[idx]\n",
        "    \n",
        "    noise = np.random.normal(0, 1, (batch_size, noise_shape))\n",
        "    \n",
        "    gen_imgs = generator.predict(noise)\n",
        "\n",
        "    #Training Discriminator\n",
        "    d_loss_real = discriminator.train_on_batch(imgs, valid)\n",
        "    d_loss_fake = discriminator.train_on_batch(gen_imgs, fakes)\n",
        "    d_loss = 0.5*np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "    #Training Generator\n",
        "    g_loss = combined.train_on_batch(noise, valid)\n",
        "\n",
        "    #Progress\n",
        "    print(\"epoch: \" + str(epoch+1) + \" \" + \"D_Loss = \" + str(d_loss[0]) + \" \" + \"acc: \" + str(d_loss[1]*100) + \" \" +  \"G_Loss = \" + str(g_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWNd6a2hCHH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(64, 90000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt-S5cnid3TL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir 'images'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIO6ER34FCaS",
        "colab_type": "text"
      },
      "source": [
        "# Generating Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoM_y_iS9hoL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_imgs(epoch):\n",
        "  r, c = 3, 3\n",
        "  noise = np.random.normal(0, 1, (r * c, noise_shape))\n",
        "  gen_imgs = generator.predict(noise)\n",
        "\n",
        "  # Rescale images 0 - 1\n",
        "  gen_imgs = (1/2.5) * gen_imgs + 0.5\n",
        "\n",
        "  fig, axs = plt.subplots(r, c)\n",
        "  cnt = 0\n",
        "  for i in range(r):\n",
        "      for j in range(c):\n",
        "          axs[i,j].imshow(gen_imgs[cnt, :,:,:])\n",
        "          axs[i,j].axis('off')\n",
        "          cnt += 1\n",
        "  fig.savefig(\"images/%d.png\" % epoch)\n",
        "  plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wQ3OHmeXzmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_imgs(100000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQ9WNTLTGejh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}