{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DC_GANs_for_3_channel_images_deeper_network_with_noise_in_real_and_fake_labels.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "aLx4oMnH2StW"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLx4oMnH2StW",
        "colab_type": "text"
      },
      "source": [
        "# Downloading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRcpwKBgIA1M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3\" --header=\"Accept-Language: en-US,en;q=0.9\" --header=\"Referer: https://www.kaggle.com/\" --header=\"Cookie: cuntwars_user_id=vFHd55haQ6; _ga=GA1.3.1069537971.1546461315; __utma=68291539.1069537971.1546461315.1547285225.1547285225.1; __utmz=68291539.1547285225.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none)\" --header=\"Connection: keep-alive\" \"https://storage.googleapis.com/kaggle-datasets/122892/296485/celebrities-100k.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1559843799&Signature=gDVXcmx7lYqZ2ymqdvbDSyoRcdrqRE1bZkCYM7H%2B%2BHcz%2Foprom8KXFaAzT01vRKcW8Qm2zpBaN0qPV%2F%2FcH8odomoOvLMyMPnEBqfOU3ZdhZe%2BL554cXnqbgIxVnPHdzFHpNuS%2BsRM93ZhavEGHJqbhGFXb%2BSozbD7zL9xWj7Ko8svymeU5odFKZ%2FBxtVElgMzecg34khwCKJlx09NAzehn3oS%2BZtqwHNXxix8JL1UCVq3FCRxHMe1sTBW6yyrWXOmhDu%2F0DwgBrxHu90EMlR5JJypwPq5jtHPtE79nbQwB2N5wUdupw2mXFNje3gqD1%2B47qrlnX82SRYHQ%2F%2BPzmeFA%3D%3D\" -O \"celebrities-100k.zip\" -c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peLSfzZ50cKc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip 'celebrities-100k.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzWjbAyF8JfI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip '100k.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-bS75jG2XwW",
        "colab_type": "text"
      },
      "source": [
        "# Building Model and Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1IStnHTXPXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir 'Model Architecture images_deeper_network_with_noise_in_real_and_fake_labels'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS-Fe5h42Php",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "from keras.layers import Dense, Dropout, Flatten, Reshape, Input, BatchNormalization, Activation, ZeroPadding2D, MaxPooling2D, AveragePooling2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Model, Sequential\n",
        "from keras.optimizers import Adam\n",
        "from PIL import Image\n",
        "from keras.utils import plot_model\n",
        "from numpy import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sys\n",
        "import os as os\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJdEXvLC2kzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "noise_shape = 100\n",
        "nh = 64\n",
        "nw = 64\n",
        "nc = 3\n",
        "img_shape = (nh, nw, nc)\n",
        "\n",
        "# Carefully chosen parameters \n",
        "d_opt = Adam(lr = 0.00004, beta_1 = 0.5)\n",
        "g_opt = Adam(lr = 0.0002, beta_1 = 0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeyQN08j3F1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_generator():\n",
        "  \n",
        "  model = Sequential()\n",
        "  model.add(Dense(128 * 4 * 4, input_dim = noise_shape, name = 'Dense_Layer'))\n",
        "  model.add(LeakyReLU(alpha=0.2, name = 'Leaky_ReLU_Dense'))\n",
        "  model.add(Reshape((4, 4, 128) , name = \"Reshape_Layer\"))\n",
        "  \n",
        "  #\n",
        "  model.add(Conv2D(128, kernel_size=3,  padding=\"same\", name = \"Conv_Layer\"))\n",
        "  model.add(LeakyReLU(alpha=0.2, name = 'Leaky_ReLU'))\n",
        "  model.add(BatchNormalization(momentum=0.8, name = \"Batch_Norm\"))\n",
        "  #\n",
        "  \n",
        "  model.add(UpSampling2D(name = 'UpSampling_Layer_2'))\n",
        "  model.add(Conv2D(64, kernel_size=3,  padding=\"same\", name = \"Conv_Layer_2\"))\n",
        "  model.add(LeakyReLU(alpha=0.2, name = 'Leaky_ReLU_2'))\n",
        "  model.add(BatchNormalization(momentum=0.8, name = \"Batch_Norm_2\"))\n",
        "  \n",
        "  model.add(UpSampling2D(name = 'UpSampling_Layer_3'))\n",
        "  model.add(Conv2D(32, kernel_size=3, padding=\"same\", name = 'Conv_Layer_3'))\n",
        "  model.add(LeakyReLU(alpha=0.2, name = 'Leaky_ReLU_3'))\n",
        "  model.add(BatchNormalization(momentum=0.8, name = 'Batch_Norm_3'))\n",
        "  \n",
        "  model.add(UpSampling2D(name = 'UpSampling_Layer_4'))\n",
        "  model.add(Conv2D(16, kernel_size=3, padding=\"same\", name = 'Conv_Layer_4'))\n",
        "  model.add(LeakyReLU(alpha=0.2, name = 'Leaky_ReLU_4'))\n",
        "  model.add(BatchNormalization(momentum=0.8, name = 'Batch_Norm_4'))\n",
        "  \n",
        "  model.add(UpSampling2D(name = 'UpSampling_Layer_5'))\n",
        "  model.add(Conv2D(8, kernel_size=3, padding=\"same\", name = 'Conv_Layer_5'))\n",
        "  model.add(LeakyReLU(alpha=0.2, name = 'Leaky_ReLU_5'))\n",
        "  model.add(BatchNormalization(momentum=0.8, name = 'Batch_Norm_5'))\n",
        "  \n",
        "  model.add(Conv2D(4, kernel_size=3, padding=\"same\", name = 'Conv_Layer_6'))\n",
        "  model.add(LeakyReLU(alpha=0.2, name = 'Leaky_ReLU_6'))\n",
        "  model.add(BatchNormalization(momentum=0.8, name = 'Batch_Norm_6'))\n",
        "  \n",
        "  model.add(Conv2D(3, kernel_size=3, padding=\"same\", name = 'Conv_Layer_7'))\n",
        "  model.add(LeakyReLU(alpha=0.2, name = 'Leaky_ReLU_7'))\n",
        "  model.add(BatchNormalization(momentum=0.8, name = 'Batch_Norm_7'))\n",
        "  model.add(Activation(\"tanh\"))\n",
        "   \n",
        "  print(model.summary())\n",
        "  \n",
        "  #Picturizing model\n",
        "  plot_model(model, to_file='Model Architecture images_deeper_network_with_noise_in_real_and_fake_labels/generator.png')\n",
        "  \n",
        "  noise = Input(shape = (noise_shape,))\n",
        "  img = model(noise)\n",
        "  \n",
        "  return Model(inputs = noise, outputs = img, name = \"Generator\")\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6QaylpG3swT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator = build_generator()\n",
        "generator.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwqnDYfm3uXu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_discriminator():\n",
        "  model = Sequential()\n",
        "  \n",
        "  model.add(Conv2D(4, kernel_size=3, input_shape = img_shape, padding=\"same\", name = 'Conv_Layer'))\n",
        "  model.add(LeakyReLU(alpha=0.2, name = 'Leaky_ReLU'))\n",
        "  model.add(BatchNormalization(momentum=0.8, name = 'Batch_Norm'))\n",
        "  \n",
        "  model.add(Conv2D(8, kernel_size=3, strides = 2, name = 'Conv_Layer_2'))\n",
        "  model.add(LeakyReLU(alpha=0.2, name = 'Leaky_ReLU_2'))\n",
        "  model.add(BatchNormalization(momentum=0.8, name = 'Batch_Norm_2'))\n",
        "  \n",
        "  model.add(Conv2D(16, kernel_size=3, strides = 2, name = 'Conv_Layer_3'))\n",
        "  model.add(LeakyReLU(alpha=0.2, name = 'Leaky_ReLU_3'))\n",
        "  model.add(BatchNormalization(momentum=0.8, name = 'Batch_Norm_3'))\n",
        "  \n",
        "  model.add(Conv2D(32, kernel_size=3, strides = 2, name = 'Conv_Layer_4'))\n",
        "  model.add(LeakyReLU(alpha=0.2, name = 'Leaky_ReLU_4'))\n",
        "  model.add(BatchNormalization(momentum=0.8, name = 'Batch_Norm_4'))\n",
        "  \n",
        "  model.add(Conv2D(64, kernel_size=2, strides = 1, name = 'Conv_Layer_5'))\n",
        "  model.add(LeakyReLU(alpha=0.2, name = 'Leaky_ReLU_5'))\n",
        "  model.add(BatchNormalization(momentum=0.8, name = 'Batch_Norm_5'))\n",
        "  \n",
        "  #\n",
        "  model.add(Conv2D(128, kernel_size=2, strides = 1, name = 'Conv_Layer_6'))\n",
        "  model.add(LeakyReLU(alpha=0.2, name = 'Leaky_ReLU_6'))\n",
        "  model.add(BatchNormalization(momentum=0.8, name = 'Batch_Norm_6'))\n",
        "  #\n",
        "  \n",
        "  model.add(Flatten(name = 'Flatten'))\n",
        "  model.add(Dense(512, name = 'Dense_Layer'))\n",
        "  model.add(LeakyReLU(alpha=0.2, name = 'Leaky_ReLU_7'))\n",
        "  \n",
        "  model.add(Dense(256, name = 'Dense_Layer_2'))\n",
        "  model.add(LeakyReLU(alpha=0.2, name = 'Leaky_ReLU_8'))\n",
        "  model.add(Dense(1, activation='sigmoid', name = 'Dense_with_Sigmoid'))\n",
        "  \n",
        "  print(model.summary())\n",
        "  \n",
        "  #Picturizing model\n",
        "  plot_model(model, to_file='Model Architecture images_deeper_network_with_noise_in_real_and_fake_labels/discriminator.png')\n",
        "  \n",
        "  img = Input(shape = img_shape)\n",
        "  validity = model(img)\n",
        "  \n",
        "  return Model(inputs = img, outputs = validity, name = 'Discriminator')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwqJneJS5Wpi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discriminator = build_discriminator()\n",
        "discriminator.summary()\n",
        "\n",
        "discriminator.compile(loss = 'binary_crossentropy', optimizer = d_opt, metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8NZedcg5W4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z = Input(shape = (noise_shape,), name = \"Input_Noise\")\n",
        "img = generator(z)\n",
        "\n",
        "discriminator.trainable = False\n",
        "valid = discriminator(img)\n",
        "\n",
        "combined = Model(z, valid)\n",
        "combined.compile(loss = 'binary_crossentropy', optimizer = g_opt)\n",
        "combined.summary()\n",
        "\n",
        "#Picturizing model\n",
        "plot_model(combined, to_file='Model Architecture images_deeper_network_with_noise_in_real_and_fake_labels/combined.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpHkFPDy5cTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(generator.summary())\n",
        "print(discriminator.summary())\n",
        "print(combined.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiYeYnjHIdQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batch(width, height):\n",
        "  image_files = os.listdir('/content/100k/')\n",
        "  data_batch = []\n",
        "  \n",
        "  for file in image_files:\n",
        "    img = Image.open('/content/100k/'+file).resize([width, height])\n",
        "    data_batch.append(np.array(img.convert('RGB')))\n",
        "    \n",
        "  data_batch = np.array(data_batch)\n",
        "  print(data_batch.shape)\n",
        "  return data_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYiMGq1g5dwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(batch_size = 128, epochs = 15830):\n",
        "  \n",
        "  \n",
        "  valid = np.ones((batch_size, 1))\n",
        "  fakes = np.ones((batch_size, 1))\n",
        "  \n",
        "  X_train = get_batch(nw, nh)\n",
        "  X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
        "  \n",
        "  for epoch in range(epochs):\n",
        "    \n",
        "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "    imgs = X_train[idx]\n",
        "    \n",
        "    noise = np.random.normal(0, 1, (batch_size, noise_shape))\n",
        "    \n",
        "    gen_imgs = generator.predict(noise)\n",
        "\n",
        "    #Training Discriminator\n",
        "    d_loss_real = discriminator.train_on_batch(imgs, valid*random.uniform(0.9, 1.0))\n",
        "    d_loss_fake = discriminator.train_on_batch(gen_imgs, fakes*random.uniform(0.0, 0.1))\n",
        "    d_loss = 0.5*np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "    #Training Generator\n",
        "    g_loss = combined.train_on_batch(noise, valid)\n",
        "\n",
        "    #Progress\n",
        "    if (epoch+1)%50 == 0:\n",
        "      print(\"epoch: \" + str(epoch+1) + \" \" + \"D_Loss = \" + str(d_loss[0]) + \" \" + \"acc: \" + str(d_loss[1]*100) + \" \" +  \"G_Loss = \" + str(g_loss))\n",
        "    \n",
        "    if (epoch+1)%10000 == 0 or epoch+1 == 1000:\n",
        "      save_imgs(epoch+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt-S5cnid3TL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir 'images_deeper_network_with_noise_in_real_and_fake_labels'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoM_y_iS9hoL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_imgs(epoch):\n",
        "  r, c = 3, 3\n",
        "  noise = np.random.normal(0, 1, (r * c, noise_shape))\n",
        "  gen_imgs = generator.predict(noise)\n",
        "\n",
        "  # Rescale images 0 - 1\n",
        "  gen_imgs = (1/2.5) * gen_imgs + 0.5\n",
        "  \n",
        "  fig, axs = plt.subplots(r, c)\n",
        "  cnt = 0\n",
        "  for i in range(r):\n",
        "      for j in range(c):\n",
        "          axs[i,j].imshow(gen_imgs[cnt, :,:,:])\n",
        "          axs[i,j].axis('off')\n",
        "          cnt += 1\n",
        "  fig.savefig(\"images_deeper_network_with_noisein_real_and_fake_labels/%d.png\" % epoch)\n",
        "  plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW63Er23vTwq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#generator.load_weights('generator.h5')\n",
        "#discriminator.load_weights('discriminator.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWNd6a2hCHH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(128, 100000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIO6ER34FCaS",
        "colab_type": "text"
      },
      "source": [
        "# Generating Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wQ3OHmeXzmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_imgs(100002)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQ9WNTLTGejh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "noise = np.random.normal(0, 1, (1, noise_shape))\n",
        "gen_imgs = generator.predict(noise)\n",
        "gen_imgs = gen_imgs*(1/2.5) + 0.5\n",
        "\n",
        "plt.imshow(gen_imgs.reshape((64 , 64, 3)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkPmDGDPPyyh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator.save_weights('generator.h5')\n",
        "discriminator.save_weights('discriminator.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Skoi-QEmyfLt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}